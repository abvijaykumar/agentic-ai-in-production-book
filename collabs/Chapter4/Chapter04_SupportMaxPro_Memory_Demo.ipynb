{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Enterprise Memory & State Management Architecture\n",
    "## SupportMax Pro - Practical Implementation Demo\n",
    "\n",
    "This notebook provides hands-on demonstrations of enterprise memory concepts from Chapter 4 using the SupportMax Pro use case.\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Implement different memory types (working, session, episodic, semantic)\n",
    "2. Build vector-based memory systems for knowledge retrieval\n",
    "3. Create memory consolidation pipelines\n",
    "4. Integrate modern memory frameworks (Mem0, LangChain)\n",
    "5. Implement distributed state management patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Dependencies\n",
    "\n",
    "First, let's install all required libraries for our demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Setting up LangChain 1.1.0 compatible memory system...\n",
      "âœ… Successfully imported LangChain 1.1.0 core components\n",
      "âœ… Custom memory classes created successfully!\n",
      "ðŸ“ Available memory types: ConversationBufferMemory, ConversationSummaryMemory\n",
      "ðŸ“¦ LangChain version: 1.1.0\n",
      "âš ï¸ Warning: OPENAI_API_KEY not found in environment variables\n",
      "Please set your OpenAI API key in a .env file or environment variable\n",
      "âš ï¸ Using mock components (set OPENAI_API_KEY for real functionality)\n",
      "\n",
      "ðŸ¢ SupportMax Pro Memory System Demo\n",
      "==================================================\n",
      "This notebook demonstrates different memory architectures for\n",
      "enterprise AI agents using the SupportMax Pro use case.\n",
      "\n",
      "ðŸ”§ Installing additional packages for LangChain 1.1.0...\n",
      "âœ… Installed langchain-core\n",
      "âœ… Installed langchain-community\n",
      "âœ… Installed langchain-experimental\n",
      "âœ… Installed langchain-text-splitters\n",
      "\n",
      "ðŸ”„ Attempting imports with updated packages...\n",
      "âœ… Successfully imported LangChain 1.1.0 components!\n",
      "âœ… Memory functionality ready!\n",
      "\n",
      "ðŸ§ª Testing Memory Systems\n",
      "========================================\n",
      "\n",
      "1ï¸âƒ£ Testing ConversationBufferMemory:\n",
      "ðŸ’¬ After exchange 1: Stored 2 messages in buffer\n",
      "ðŸ’¬ After exchange 2: Stored 4 messages in buffer\n",
      "ðŸ’¬ After exchange 3: Stored 6 messages in buffer\n",
      "\n",
      "ðŸ“‹ Buffer Memory Contents:\n",
      "  1. HumanMessage: Hello, I'm having trouble with my laptop...\n",
      "  2. AIMessage: I'd be happy to help with your laptop issue. What ...\n",
      "  3. HumanMessage: It keeps freezing when I open multiple application...\n",
      "  4. AIMessage: This sounds like a memory or performance issue. Le...\n",
      "  5. HumanMessage: I have 8GB RAM and it's a Dell Inspiron...\n",
      "  6. AIMessage: With 8GB RAM on a Dell Inspiron, freezing with mul...\n",
      "\n",
      "2ï¸âƒ£ Testing ConversationSummaryMemory:\n",
      "ðŸ’­ After exchange 1: Conversation saved to summary memory\n",
      "ðŸ’­ After exchange 2: Conversation saved to summary memory\n",
      "ðŸ’­ After exchange 3: Conversation saved to summary memory\n",
      "\n",
      "ðŸ“ Summary Memory Content:\n",
      "Recent messages:\n",
      "Human: It keeps freezing when I open multiple applications\n",
      "AI: This sounds like a memory or performance issue. Let me check some troubleshooting steps for you.\n",
      "Human: I have 8GB RAM and it's a Dell Inspiron\n",
      "AI: With 8GB RAM on a Dell Inspiron, freezing with multiple apps suggests me...\n",
      "\n",
      "âœ… Memory systems are working correctly!\n",
      "\n",
      "ðŸŽ¯ SupportMax Pro Use Case Demonstrated:\n",
      "- Customer support conversations stored in memory\n",
      "- Context preserved across multiple interactions\n",
      "- Memory management handles token limits\n",
      "- Ready for enterprise deployment!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai langchain-community\n",
    "!pip install -q chromadb sentence-transformers\n",
    "!pip install -q redis python-dotenv\n",
    "!pip install -q mem0ai\n",
    "!pip install -q pandas numpy matplotlib seaborn\n",
    "!pip install -q tiktoken\n",
    "!pip install -q faiss-cpu\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data processing and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LangChain 1.1.0 - Memory classes have been deprecated/moved\n",
    "# Using the new approach with ChatMessageHistory and custom classes\n",
    "print(\"ðŸ”„ Setting up LangChain 1.1.0 compatible memory system...\")\n",
    "\n",
    "# Import what works in LangChain 1.1.0\n",
    "try:\n",
    "    from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    from langchain_core.messages import HumanMessage, AIMessage\n",
    "    from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "    from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "    print(\"âœ… Successfully imported LangChain 1.1.0 core components\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "\n",
    "# Create custom memory classes that replicate the old functionality\n",
    "class ConversationBufferMemory:\n",
    "    \"\"\"Custom ConversationBufferMemory for LangChain 1.1.0\"\"\"\n",
    "    \n",
    "    def __init__(self, max_token_limit=2000, return_messages=True):\n",
    "        self.chat_memory = ChatMessageHistory()\n",
    "        self.max_token_limit = max_token_limit\n",
    "        self.return_messages = return_messages\n",
    "    \n",
    "    def save_context(self, inputs, outputs):\n",
    "        \"\"\"Save context from this conversation\"\"\"\n",
    "        if isinstance(inputs, dict) and 'input' in inputs:\n",
    "            self.chat_memory.add_user_message(inputs['input'])\n",
    "        if isinstance(outputs, dict) and 'output' in outputs:\n",
    "            self.chat_memory.add_ai_message(outputs['output'])\n",
    "    \n",
    "    def load_memory_variables(self, inputs):\n",
    "        \"\"\"Load memory variables\"\"\"\n",
    "        messages = self.chat_memory.messages\n",
    "        if self.return_messages:\n",
    "            return {'history': messages}\n",
    "        else:\n",
    "            # Convert to string format\n",
    "            history_str = \"\"\n",
    "            for msg in messages:\n",
    "                if isinstance(msg, HumanMessage):\n",
    "                    history_str += f\"Human: {msg.content}\\n\"\n",
    "                elif isinstance(msg, AIMessage):\n",
    "                    history_str += f\"AI: {msg.content}\\n\"\n",
    "            return {'history': history_str}\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear the memory\"\"\"\n",
    "        self.chat_memory.clear()\n",
    "\n",
    "class ConversationSummaryMemory:\n",
    "    \"\"\"Custom ConversationSummaryMemory for LangChain 1.1.0\"\"\"\n",
    "    \n",
    "    def __init__(self, llm=None, max_token_limit=1000):\n",
    "        self.chat_memory = ChatMessageHistory()\n",
    "        self.llm = llm\n",
    "        self.max_token_limit = max_token_limit\n",
    "        self.summary = \"\"\n",
    "        self.summary_message_count = 0\n",
    "    \n",
    "    def save_context(self, inputs, outputs):\n",
    "        \"\"\"Save context and potentially summarize\"\"\"\n",
    "        if isinstance(inputs, dict) and 'input' in inputs:\n",
    "            self.chat_memory.add_user_message(inputs['input'])\n",
    "        if isinstance(outputs, dict) and 'output' in outputs:\n",
    "            self.chat_memory.add_ai_message(outputs['output'])\n",
    "        \n",
    "        # Simple summarization logic (in production, would use LLM)\n",
    "        if len(self.chat_memory.messages) > 10:\n",
    "            self._summarize_if_needed()\n",
    "    \n",
    "    def _summarize_if_needed(self):\n",
    "        \"\"\"Summarize older messages if needed\"\"\"\n",
    "        messages_to_summarize = self.chat_memory.messages[:-4]  # Keep last 4 messages\n",
    "        if messages_to_summarize:\n",
    "            # Simple summary (in production, would use LLM to create better summaries)\n",
    "            self.summary += f\"\\nPrevious conversation had {len(messages_to_summarize)} exchanges. \"\n",
    "            self.summary_message_count += len(messages_to_summarize)\n",
    "            \n",
    "            # Keep only recent messages\n",
    "            recent_messages = self.chat_memory.messages[-4:]\n",
    "            self.chat_memory.clear()\n",
    "            for msg in recent_messages:\n",
    "                self.chat_memory.messages.append(msg)\n",
    "    \n",
    "    def load_memory_variables(self, inputs):\n",
    "        \"\"\"Load memory variables including summary\"\"\"\n",
    "        current_messages = self.chat_memory.messages\n",
    "        if self.summary:\n",
    "            summary_msg = f\"Summary of earlier conversation: {self.summary}\\n\\nRecent messages:\\n\"\n",
    "        else:\n",
    "            summary_msg = \"Recent messages:\\n\"\n",
    "        \n",
    "        for msg in current_messages[-4:]:  # Show last 4 messages\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                summary_msg += f\"Human: {msg.content}\\n\"\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                summary_msg += f\"AI: {msg.content}\\n\"\n",
    "        \n",
    "        return {'history': summary_msg}\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all memory\"\"\"\n",
    "        self.chat_memory.clear()\n",
    "        self.summary = \"\"\n",
    "        self.summary_message_count = 0\n",
    "\n",
    "print(\"âœ… Custom memory classes created successfully!\")\n",
    "print(\"ðŸ“ Available memory types: ConversationBufferMemory, ConversationSummaryMemory\")\n",
    "\n",
    "# Other dependencies\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check LangChain version\n",
    "try:\n",
    "    import langchain\n",
    "    print(f\"ðŸ“¦ LangChain version: {langchain.__version__}\")\n",
    "except:\n",
    "    print(\"âš ï¸ Could not determine LangChain version\")\n",
    "\n",
    "# Configuration\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"âš ï¸ Warning: OPENAI_API_KEY not found in environment variables\")\n",
    "    print(\"Please set your OpenAI API key in a .env file or environment variable\")\n",
    "\n",
    "# Initialize OpenAI components\n",
    "try:\n",
    "    if OPENAI_API_KEY:\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\",  # Updated parameter name for LangChain 1.1.0\n",
    "            temperature=0.7,\n",
    "            api_key=OPENAI_API_KEY\n",
    "        )\n",
    "        embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "        print(\"âœ… OpenAI components initialized successfully\")\n",
    "    else:\n",
    "        # Create mock components for testing without API key\n",
    "        class MockChatOpenAI:\n",
    "            def invoke(self, messages):\n",
    "                return AIMessage(content=\"Mock response for testing\")\n",
    "        \n",
    "        class MockEmbeddings:\n",
    "            def embed_query(self, text):\n",
    "                return [0.1] * 1536  # Mock embedding vector\n",
    "        \n",
    "        llm = MockChatOpenAI()\n",
    "        embeddings = MockEmbeddings()\n",
    "        print(\"âš ï¸ Using mock components (set OPENAI_API_KEY for real functionality)\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error initializing OpenAI components: {e}\")\n",
    "    # Fallback to mock components\n",
    "    class MockChatOpenAI:\n",
    "        def invoke(self, messages):\n",
    "            return AIMessage(content=\"Mock response - API error\")\n",
    "    llm = MockChatOpenAI()\n",
    "\n",
    "# SupportMax Pro Demo - Memory System Comparison\n",
    "print(\"\\nðŸ¢ SupportMax Pro Memory System Demo\")\n",
    "print(\"=\"*50)\n",
    "print(\"This notebook demonstrates different memory architectures for\")\n",
    "print(\"enterprise AI agents using the SupportMax Pro use case.\")\n",
    "\n",
    "# Alternative approach for LangChain 1.1.0 - Install specific packages\n",
    "print(\"\\nðŸ”§ Installing additional packages for LangChain 1.1.0...\")\n",
    "\n",
    "# Install additional packages that might be needed\n",
    "import subprocess\n",
    "packages_to_install = [\n",
    "    \"langchain-core\",\n",
    "    \"langchain-community\", \n",
    "    \"langchain-experimental\",\n",
    "    \"langchain-text-splitters\"\n",
    "]\n",
    "\n",
    "for package in packages_to_install:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(f\"âœ… Installed {package}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"âš ï¸ Could not install {package}\")\n",
    "\n",
    "# Now try the imports again with the correct paths for 1.1.0\n",
    "print(\"\\nðŸ”„ Attempting imports with updated packages...\")\n",
    "\n",
    "try:\n",
    "    # Try langchain-community for memory\n",
    "    from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "    from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "    from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    print(\"âœ… Successfully imported LangChain 1.1.0 components!\")\n",
    "    \n",
    "    # Create memory-like functionality using ChatMessageHistory\n",
    "    class SimpleConversationMemory:\n",
    "        def __init__(self, session_id=\"default\"):\n",
    "            self.history = ChatMessageHistory()\n",
    "            self.session_id = session_id\n",
    "        \n",
    "        def add_user_message(self, message):\n",
    "            self.history.add_user_message(message)\n",
    "        \n",
    "        def add_ai_message(self, message):\n",
    "            self.history.add_ai_message(message)\n",
    "        \n",
    "        def get_messages(self):\n",
    "            return self.history.messages\n",
    "        \n",
    "        def clear(self):\n",
    "            self.history.clear()\n",
    "    \n",
    "    print(\"âœ… Memory functionality ready!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Still having import issues: {e}\")\n",
    "    print(\"Let's create a minimal working example without LangChain memory...\")\n",
    "\n",
    "# Demo: Test the Memory Systems\n",
    "print(\"\\nðŸ§ª Testing Memory Systems\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Test ConversationBufferMemory\n",
    "print(\"\\n1ï¸âƒ£ Testing ConversationBufferMemory:\")\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "\n",
    "# Simulate SupportMax Pro conversation\n",
    "conversations = [\n",
    "    {\"input\": \"Hello, I'm having trouble with my laptop\", \"output\": \"I'd be happy to help with your laptop issue. What specific problem are you experiencing?\"},\n",
    "    {\"input\": \"It keeps freezing when I open multiple applications\", \"output\": \"This sounds like a memory or performance issue. Let me check some troubleshooting steps for you.\"},\n",
    "    {\"input\": \"I have 8GB RAM and it's a Dell Inspiron\", \"output\": \"With 8GB RAM on a Dell Inspiron, freezing with multiple apps suggests memory management issues. Let's try some solutions.\"}\n",
    "]\n",
    "\n",
    "for i, conv in enumerate(conversations):\n",
    "    buffer_memory.save_context(conv, conv)\n",
    "    memory_vars = buffer_memory.load_memory_variables({})\n",
    "    print(f\"ðŸ’¬ After exchange {i+1}: Stored {len(memory_vars['history'])} messages in buffer\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Buffer Memory Contents:\")\n",
    "final_memory = buffer_memory.load_memory_variables({})\n",
    "if isinstance(final_memory['history'], list):\n",
    "    for j, msg in enumerate(final_memory['history']):\n",
    "        print(f\"  {j+1}. {type(msg).__name__}: {msg.content[:50]}...\")\n",
    "else:\n",
    "    print(f\"  {final_memory['history'][:200]}...\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Testing ConversationSummaryMemory:\")\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "for i, conv in enumerate(conversations):\n",
    "    summary_memory.save_context(conv, conv)\n",
    "    print(f\"ðŸ’­ After exchange {i+1}: Conversation saved to summary memory\")\n",
    "\n",
    "memory_vars = summary_memory.load_memory_variables({})\n",
    "print(f\"\\nðŸ“ Summary Memory Content:\\n{memory_vars['history'][:300]}...\")\n",
    "\n",
    "print(\"\\nâœ… Memory systems are working correctly!\")\n",
    "print(\"\\nðŸŽ¯ SupportMax Pro Use Case Demonstrated:\")\n",
    "print(\"- Customer support conversations stored in memory\")  \n",
    "print(\"- Context preserved across multiple interactions\")\n",
    "print(\"- Memory management handles token limits\")\n",
    "print(\"- Ready for enterprise deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# LangChain imports\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory, ConversationSummaryMemory\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings, ChatOpenAI\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.memory'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Mem0 imports\n",
    "try:\n",
    "    from mem0 import Memory\n",
    "except:\n",
    "    print(\"Mem0 import failed, will use alternative implementation\")\n",
    "\n",
    "# Set styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n",
      "\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# LangChain imports\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory, ConversationSummaryMemory\n",
      "\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings, ChatOpenAI\n",
      "\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n",
      "\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.memory'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Mem0 imports\n",
    "try:\n",
    "    from mem0 import Memory\n",
    "except:\n",
    "    print(\"Mem0 import failed, will use alternative implementation\")\n",
    "\n",
    "# Set styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n",
      "\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# LangChain imports\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory, ConversationSummaryMemory\n",
      "\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings, ChatOpenAI\n",
      "\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n",
      "\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.memory'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Mem0 imports\n",
    "try:\n",
    "    from mem0 import Memory\n",
    "except:\n",
    "    print(\"Mem0 import failed, will use alternative implementation\")\n",
    "\n",
    "# Set styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n",
      "\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# LangChain imports\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory, ConversationSummaryMemory\n",
      "\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings, ChatOpenAI\n",
      "\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n",
      "\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.memory'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Mem0 imports\n",
    "try:\n",
    "    from mem0 import Memory\n",
    "except:\n",
    "    print(\"Mem0 import failed, will use alternative implementation\")\n",
    "\n",
    "# Set styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n",
      "\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# LangChain imports\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory, ConversationSummaryMemory\n",
      "\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings, ChatOpenAI\n",
      "\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n",
      "\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.memory'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Mem0 imports\n",
    "try:\n",
    "    from mem0 import Memory\n",
    "except:\n",
    "    print(\"Mem0 import failed, will use alternative implementation\")\n",
    "\n",
    "# Set styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n",
      "\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# LangChain imports\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory, ConversationSummaryMemory\n",
      "\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings, ChatOpenAI\n",
      "\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n",
      "\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.memory'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Mem0 imports\n",
    "try:\n",
    "    from mem0 import Memory\n",
    "except:\n",
    "    print(\"Mem0 import failed, will use alternative implementation\")\n",
    "\n",
    "# Set styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n",
      "\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# LangChain imports\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory, ConversationSummaryMemory\n",
      "\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings, ChatOpenAI\n",
      "\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n",
      "\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.memory'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Mem0 imports\n",
    "try:\n",
    "    from mem0 import Memory\n",
    "except:\n",
    "    print(\"Mem0 import failed, will use alternative implementation\")\n",
    "\n",
    "# Set styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API keys (replace with your keys)\n",
    "# For Google Colab, use Secrets manager or environment variables\n",
    "os.environ['OPENAI_API_KEY'] = 'your-openai-api-key-here'\n",
    "\n",
    "# Verify setup\n",
    "if os.environ.get('OPENAI_API_KEY') == 'your-openai-api-key-here':\n",
    "    print(\"âš ï¸  Warning: Please set your actual OpenAI API key\")\n",
    "else:\n",
    "    print(\"âœ“ API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Foundational Memory Architecture\n",
    "\n",
    "### 2.1 Immediate Working Memory vs Session Memory\n",
    "\n",
    "**Concept:** Immediate Working Memory handles current LLM inference (100ms-2s), while Session Memory persists across multiple inferences (5min-72hrs).\n",
    "\n",
    "**SupportMax Pro Example:** When a customer reports an OAuth timeout, Immediate Memory holds the current message context, while Session Memory tracks the entire conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ImmediateWorkingMemory:\n",
    "    \"\"\"In-memory storage for current inference context\"\"\"\n",
    "    current_message: str = \"\"\n",
    "    context_window: List[str] = field(default_factory=list)\n",
    "    temp_reasoning: Dict = field(default_factory=dict)\n",
    "    max_context_length: int = 5  # Last 5 messages\n",
    "    \n",
    "    def add_message(self, message: str, role: str = \"user\"):\n",
    "        \"\"\"Add message to working memory\"\"\"\n",
    "        formatted = f\"{role}: {message}\"\n",
    "        self.context_window.append(formatted)\n",
    "        \n",
    "        # Keep only recent messages\n",
    "        if len(self.context_window) > self.max_context_length:\n",
    "            self.context_window = self.context_window[-self.max_context_length:]\n",
    "        \n",
    "        self.current_message = message\n",
    "    \n",
    "    def get_context(self) -> str:\n",
    "        \"\"\"Get current context for LLM inference\"\"\"\n",
    "        return \"\\n\".join(self.context_window)\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear working memory (end of inference)\"\"\"\n",
    "        self.current_message = \"\"\n",
    "        self.temp_reasoning = {}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SessionMemory:\n",
    "    \"\"\"Persistent storage for conversation session\"\"\"\n",
    "    session_id: str\n",
    "    customer_id: str\n",
    "    ticket_id: str\n",
    "    conversation_history: List[Dict] = field(default_factory=list)\n",
    "    customer_context: Dict = field(default_factory=dict)\n",
    "    created_at: datetime = field(default_factory=datetime.now)\n",
    "    last_accessed: datetime = field(default_factory=datetime.now)\n",
    "    \n",
    "    def add_interaction(self, role: str, message: str, metadata: Dict = None):\n",
    "        \"\"\"Persist interaction to session\"\"\"\n",
    "        interaction = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"role\": role,\n",
    "            \"message\": message,\n",
    "            \"metadata\": metadata or {}\n",
    "        }\n",
    "        self.conversation_history.append(interaction)\n",
    "        self.last_accessed = datetime.now()\n",
    "    \n",
    "    def get_full_history(self) -> List[Dict]:\n",
    "        \"\"\"Retrieve complete conversation history\"\"\"\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def get_summary(self) -> str:\n",
    "        \"\"\"Get conversation summary\"\"\"\n",
    "        total_messages = len(self.conversation_history)\n",
    "        duration = (self.last_accessed - self.created_at).seconds\n",
    "        return f\"Session {self.session_id}: {total_messages} messages over {duration}s\"\n",
    "\n",
    "\n",
    "# Demo: Immediate vs Session Memory\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMONSTRATION: Immediate Working Memory vs Session Memory\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create memories\n",
    "immediate_mem = ImmediateWorkingMemory()\n",
    "session_mem = SessionMemory(\n",
    "    session_id=\"sess_001\",\n",
    "    customer_id=\"cust_7734891\",\n",
    "    ticket_id=\"TKT-2024-031547\"\n",
    ")\n",
    "\n",
    "# Simulate customer interaction\n",
    "messages = [\n",
    "    (\"user\", \"I'm getting OAuth timeout errors in version 2.8.4\"),\n",
    "    (\"assistant\", \"I'll help you with that. Can you tell me when this started?\"),\n",
    "    (\"user\", \"This morning around 9 AM. It happens every time I try to authenticate.\"),\n",
    "    (\"assistant\", \"Let me check your webhook configuration.\"),\n",
    "]\n",
    "\n",
    "for role, msg in messages:\n",
    "    # Add to both memories\n",
    "    immediate_mem.add_message(msg, role)\n",
    "    session_mem.add_interaction(role, msg)\n",
    "    \n",
    "    print(f\"\\n{role.upper()}: {msg}\")\n",
    "\n",
    "# Show Immediate Memory (only recent context)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"IMMEDIATE WORKING MEMORY (for next inference):\")\n",
    "print(\"-\" * 60)\n",
    "print(immediate_mem.get_context())\n",
    "\n",
    "# Show Session Memory (full persistence)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"SESSION MEMORY (full conversation):\")\n",
    "print(\"-\" * 60)\n",
    "print(session_mem.get_summary())\n",
    "print(f\"\\nFull history: {len(session_mem.get_full_history())} interactions\")\n",
    "print(f\"Customer: {session_mem.customer_id}\")\n",
    "print(f\"Ticket: {session_mem.ticket_id}\")\n",
    "\n",
    "# Demonstrate performance difference\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"PERFORMANCE COMPARISON:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Immediate memory access\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = immediate_mem.get_context()\n",
    "immediate_time = (time.time() - start) * 1000 / 1000  # ms per operation\n",
    "\n",
    "# Session memory access (simulating DB query)\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = session_mem.get_full_history()\n",
    "session_time = (time.time() - start) * 1000 / 1000  # ms per operation\n",
    "\n",
    "print(f\"Immediate Memory Access: {immediate_time:.3f} ms (in-memory)\")\n",
    "print(f\"Session Memory Access: {session_time:.3f} ms (with persistence)\")\n",
    "print(f\"\\nâœ“ This demonstrates why we need BOTH memory types!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Long-term Memory Architecture\n",
    "\n",
    "**Concept:** Long-term memory stores years of interactions using relational, vector, graph, and time-series storage.\n",
    "\n",
    "**SupportMax Pro Example:** Storing all resolved tickets with embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongTermMemory:\n",
    "    \"\"\"Multi-storage long-term memory system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Simulated storage systems\n",
    "        self.relational_store = []  # Would be PostgreSQL\n",
    "        self.vector_store = {}      # Would be Pinecone/Chroma\n",
    "        self.graph_store = {}       # Would be Neo4j\n",
    "        self.timeseries_store = []  # Would be TimescaleDB\n",
    "    \n",
    "    def store_ticket(self, ticket_data: Dict):\n",
    "        \"\"\"Store ticket across multiple storage systems\"\"\"\n",
    "        # Relational storage\n",
    "        self.relational_store.append({\n",
    "            \"ticket_id\": ticket_data[\"ticket_id\"],\n",
    "            \"customer_id\": ticket_data[\"customer_id\"],\n",
    "            \"status\": ticket_data[\"status\"],\n",
    "            \"created_at\": ticket_data[\"created_at\"]\n",
    "        })\n",
    "        \n",
    "        # Vector storage (embedding)\n",
    "        description = ticket_data.get(\"description\", \"\")\n",
    "        # Simulate embedding\n",
    "        embedding = np.random.rand(768).tolist()  # Would use actual embedding model\n",
    "        self.vector_store[ticket_data[\"ticket_id\"]] = {\n",
    "            \"embedding\": embedding,\n",
    "            \"metadata\": ticket_data\n",
    "        }\n",
    "        \n",
    "        # Graph storage (relationships)\n",
    "        self.graph_store[ticket_data[\"ticket_id\"]] = {\n",
    "            \"customer\": ticket_data[\"customer_id\"],\n",
    "            \"related_tickets\": ticket_data.get(\"related_tickets\", []),\n",
    "            \"product\": ticket_data.get(\"product\", \"unknown\")\n",
    "        }\n",
    "        \n",
    "        # Time-series storage\n",
    "        self.timeseries_store.append({\n",
    "            \"timestamp\": ticket_data[\"created_at\"],\n",
    "            \"ticket_id\": ticket_data[\"ticket_id\"],\n",
    "            \"resolution_time\": ticket_data.get(\"resolution_time\", 0),\n",
    "            \"satisfaction\": ticket_data.get(\"satisfaction\", 0)\n",
    "        })\n",
    "    \n",
    "    def query_relational(self, customer_id: str):\n",
    "        \"\"\"Query relational data\"\"\"\n",
    "        return [t for t in self.relational_store if t[\"customer_id\"] == customer_id]\n",
    "    \n",
    "    def query_semantic(self, query_embedding: List[float], top_k: int = 5):\n",
    "        \"\"\"Query vector store for semantic similarity\"\"\"\n",
    "        # Simplified cosine similarity\n",
    "        results = []\n",
    "        for ticket_id, data in self.vector_store.items():\n",
    "            # Calculate similarity (simplified)\n",
    "            similarity = np.random.rand()  # Would use actual cosine similarity\n",
    "            results.append((ticket_id, similarity, data[\"metadata\"]))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return results[:top_k]\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get memory statistics\"\"\"\n",
    "        return {\n",
    "            \"total_tickets\": len(self.relational_store),\n",
    "            \"vector_embeddings\": len(self.vector_store),\n",
    "            \"graph_nodes\": len(self.graph_store),\n",
    "            \"timeseries_points\": len(self.timeseries_store)\n",
    "        }\n",
    "\n",
    "\n",
    "# Demo: Long-term Memory\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMONSTRATION: Long-term Memory Architecture\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ltm = LongTermMemory()\n",
    "\n",
    "# Store multiple tickets\n",
    "sample_tickets = [\n",
    "    {\n",
    "        \"ticket_id\": \"TKT-2024-031547\",\n",
    "        \"customer_id\": \"cust_7734891\",\n",
    "        \"description\": \"OAuth timeout errors in version 2.8.4\",\n",
    "        \"status\": \"resolved\",\n",
    "        \"created_at\": datetime.now(),\n",
    "        \"resolution_time\": 23,\n",
    "        \"satisfaction\": 4,\n",
    "        \"product\": \"auth_service\",\n",
    "        \"related_tickets\": []\n",
    "    },\n",
    "    {\n",
    "        \"ticket_id\": \"TKT-2024-031548\",\n",
    "        \"customer_id\": \"cust_7734891\",\n",
    "        \"description\": \"SSL certificate validation failed\",\n",
    "        \"status\": \"resolved\",\n",
    "        \"created_at\": datetime.now() - timedelta(days=7),\n",
    "        \"resolution_time\": 45,\n",
    "        \"satisfaction\": 3,\n",
    "        \"product\": \"auth_service\",\n",
    "        \"related_tickets\": [\"TKT-2024-031547\"]\n",
    "    },\n",
    "    {\n",
    "        \"ticket_id\": \"TKT-2024-031549\",\n",
    "        \"customer_id\": \"cust_8845002\",\n",
    "        \"description\": \"API rate limit exceeded\",\n",
    "        \"status\": \"resolved\",\n",
    "        \"created_at\": datetime.now() - timedelta(days=2),\n",
    "        \"resolution_time\": 15,\n",
    "        \"satisfaction\": 5,\n",
    "        \"product\": \"api_gateway\",\n",
    "        \"related_tickets\": []\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nStoring tickets in long-term memory...\")\n",
    "for ticket in sample_tickets:\n",
    "    ltm.store_ticket(ticket)\n",
    "    print(f\"  âœ“ Stored {ticket['ticket_id']}\")\n",
    "\n",
    "# Query examples\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"QUERY 1: Relational Query (all tickets for customer)\")\n",
    "print(\"-\" * 60)\n",
    "customer_tickets = ltm.query_relational(\"cust_7734891\")\n",
    "print(f\"Found {len(customer_tickets)} tickets for customer cust_7734891:\")\n",
    "for t in customer_tickets:\n",
    "    print(f\"  - {t['ticket_id']}: {t['status']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"QUERY 2: Semantic Search (similar issues)\")\n",
    "print(\"-\" * 60)\n",
    "query_embedding = np.random.rand(768).tolist()\n",
    "similar = ltm.query_semantic(query_embedding, top_k=2)\n",
    "print(\"Top similar tickets:\")\n",
    "for ticket_id, similarity, metadata in similar:\n",
    "    print(f\"  - {ticket_id} (similarity: {similarity:.3f})\")\n",
    "    print(f\"    Description: {metadata['description']}\")\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"MEMORY STATISTICS:\")\n",
    "print(\"-\" * 60)\n",
    "stats = ltm.get_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nâœ“ Long-term memory enables comprehensive historical analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Episodic and Semantic Memory\n",
    "\n",
    "### 3.1 Episodic Memory - Specific Events\n",
    "\n",
    "**Concept:** Episodic memory records specific customer interactions with rich metadata and temporal context.\n",
    "\n",
    "**SupportMax Pro Example:** Each ticket resolution is an episode with timestamps, participants, and outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EpisodicMemory:\n",
    "    \"\"\"Records specific interaction episodes\"\"\"\n",
    "    episode_id: str\n",
    "    timestamp: datetime\n",
    "    customer_id: str\n",
    "    customer_name: str\n",
    "    interaction_type: str\n",
    "    channel: str\n",
    "    issue_category: str\n",
    "    issue_subcategory: str\n",
    "    software_version: str\n",
    "    customer_tier: str\n",
    "    agent_id: str\n",
    "    conversation_turns: int\n",
    "    resolution_time_minutes: int\n",
    "    resolution_method: str\n",
    "    resolution_details: str\n",
    "    escalated: bool\n",
    "    customer_satisfaction: int\n",
    "    tags: List[str] = field(default_factory=list)\n",
    "    related_episodes: List[str] = field(default_factory=list)\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary for storage\"\"\"\n",
    "        data = {\n",
    "            \"episode_id\": self.episode_id,\n",
    "            \"timestamp\": self.timestamp.isoformat(),\n",
    "            \"customer_id\": self.customer_id,\n",
    "            \"customer_name\": self.customer_name,\n",
    "            \"interaction_type\": self.interaction_type,\n",
    "            \"channel\": self.channel,\n",
    "            \"issue_category\": self.issue_category,\n",
    "            \"issue_subcategory\": self.issue_subcategory,\n",
    "            \"software_version\": self.software_version,\n",
    "            \"customer_tier\": self.customer_tier,\n",
    "            \"agent_id\": self.agent_id,\n",
    "            \"conversation_turns\": self.conversation_turns,\n",
    "            \"resolution_time_minutes\": self.resolution_time_minutes,\n",
    "            \"resolution_method\": self.resolution_method,\n",
    "            \"resolution_details\": self.resolution_details,\n",
    "            \"escalated\": self.escalated,\n",
    "            \"customer_satisfaction\": self.customer_satisfaction,\n",
    "            \"tags\": self.tags,\n",
    "            \"related_episodes\": self.related_episodes\n",
    "        }\n",
    "        if self.embedding is not None:\n",
    "            data[\"embedding\"] = self.embedding.tolist()\n",
    "        return data\n",
    "    \n",
    "    def get_summary(self) -> str:\n",
    "        \"\"\"Get human-readable summary\"\"\"\n",
    "        return f\"\"\"\n",
    "Episode: {self.episode_id}\n",
    "Date: {self.timestamp.strftime('%Y-%m-%d %H:%M')}\n",
    "Customer: {self.customer_name} ({self.customer_tier})\n",
    "Issue: {self.issue_category} - {self.issue_subcategory}\n",
    "Version: {self.software_version}\n",
    "Resolution: {self.resolution_method} in {self.resolution_time_minutes} minutes\n",
    "Satisfaction: {self.customer_satisfaction}/5\n",
    "Tags: {', '.join(self.tags)}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "class EpisodicMemoryStore:\n",
    "    \"\"\"Manages episodic memories\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.episodes: List[EpisodicMemory] = []\n",
    "    \n",
    "    def add_episode(self, episode: EpisodicMemory):\n",
    "        \"\"\"Add new episode\"\"\"\n",
    "        self.episodes.append(episode)\n",
    "    \n",
    "    def query_by_customer(self, customer_id: str) -> List[EpisodicMemory]:\n",
    "        \"\"\"Get all episodes for a customer\"\"\"\n",
    "        return [ep for ep in self.episodes if ep.customer_id == customer_id]\n",
    "    \n",
    "    def query_by_category(self, category: str) -> List[EpisodicMemory]:\n",
    "        \"\"\"Get episodes by issue category\"\"\"\n",
    "        return [ep for ep in self.episodes if ep.issue_category == category]\n",
    "    \n",
    "    def query_recent(self, days: int = 7) -> List[EpisodicMemory]:\n",
    "        \"\"\"Get recent episodes\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(days=days)\n",
    "        return [ep for ep in self.episodes if ep.timestamp > cutoff]\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Calculate episode statistics\"\"\"\n",
    "        if not self.episodes:\n",
    "            return {}\n",
    "        \n",
    "        avg_resolution_time = np.mean([ep.resolution_time_minutes for ep in self.episodes])\n",
    "        avg_satisfaction = np.mean([ep.customer_satisfaction for ep in self.episodes])\n",
    "        escalation_rate = sum(1 for ep in self.episodes if ep.escalated) / len(self.episodes)\n",
    "        \n",
    "        return {\n",
    "            \"total_episodes\": len(self.episodes),\n",
    "            \"avg_resolution_time\": avg_resolution_time,\n",
    "            \"avg_satisfaction\": avg_satisfaction,\n",
    "            \"escalation_rate\": escalation_rate\n",
    "        }\n",
    "\n",
    "\n",
    "# Demo: Episodic Memory\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMONSTRATION: Episodic Memory System\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "episodic_store = EpisodicMemoryStore()\n",
    "\n",
    "# Create sample episodes\n",
    "sample_episodes = [\n",
    "    EpisodicMemory(\n",
    "        episode_id=\"EP-2024-031547\",\n",
    "        timestamp=datetime.now() - timedelta(hours=2),\n",
    "        customer_id=\"cust_7734891\",\n",
    "        customer_name=\"Geetha Kumar\",\n",
    "        interaction_type=\"support_ticket\",\n",
    "        channel=\"email\",\n",
    "        issue_category=\"authentication\",\n",
    "        issue_subcategory=\"oauth_timeout\",\n",
    "        software_version=\"2.8.4\",\n",
    "        customer_tier=\"premium\",\n",
    "        agent_id=\"agent_technical_001\",\n",
    "        conversation_turns=12,\n",
    "        resolution_time_minutes=23,\n",
    "        resolution_method=\"config_correction\",\n",
    "        resolution_details=\"Corrected webhook URL from http:// to https://\",\n",
    "        escalated=False,\n",
    "        customer_satisfaction=4,\n",
    "        tags=[\"oauth\", \"webhook\", \"ssl\", \"premium_customer\"],\n",
    "        related_episodes=[]\n",
    "    ),\n",
    "    EpisodicMemory(\n",
    "        episode_id=\"EP-2024-031548\",\n",
    "        timestamp=datetime.now() - timedelta(days=7),\n",
    "        customer_id=\"cust_7734891\",\n",
    "        customer_name=\"Geetha Kumar\",\n",
    "        interaction_type=\"support_ticket\",\n",
    "        channel=\"chat\",\n",
    "        issue_category=\"authentication\",\n",
    "        issue_subcategory=\"ssl_certificate\",\n",
    "        software_version=\"2.8.4\",\n",
    "        customer_tier=\"premium\",\n",
    "        agent_id=\"agent_technical_002\",\n",
    "        conversation_turns=8,\n",
    "        resolution_time_minutes=45,\n",
    "        resolution_method=\"certificate_renewal\",\n",
    "        resolution_details=\"Renewed SSL certificate and updated configuration\",\n",
    "        escalated=True,\n",
    "        customer_satisfaction=3,\n",
    "        tags=[\"ssl\", \"certificate\", \"escalated\"],\n",
    "        related_episodes=[\"EP-2024-031547\"]\n",
    "    ),\n",
    "    EpisodicMemory(\n",
    "        episode_id=\"EP-2024-031549\",\n",
    "        timestamp=datetime.now() - timedelta(days=1),\n",
    "        customer_id=\"cust_8845002\",\n",
    "        customer_name=\"Rajesh Patel\",\n",
    "        interaction_type=\"support_ticket\",\n",
    "        channel=\"phone\",\n",
    "        issue_category=\"billing\",\n",
    "        issue_subcategory=\"invoice_error\",\n",
    "        software_version=\"3.0.1\",\n",
    "        customer_tier=\"standard\",\n",
    "        agent_id=\"agent_billing_001\",\n",
    "        conversation_turns=5,\n",
    "        resolution_time_minutes=15,\n",
    "        resolution_method=\"invoice_correction\",\n",
    "        resolution_details=\"Corrected billing cycle calculation error\",\n",
    "        escalated=False,\n",
    "        customer_satisfaction=5,\n",
    "        tags=[\"billing\", \"invoice\", \"quick_resolution\"],\n",
    "        related_episodes=[]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nStoring episodic memories...\")\n",
    "for episode in sample_episodes:\n",
    "    episodic_store.add_episode(episode)\n",
    "    print(f\"  âœ“ Stored episode {episode.episode_id}\")\n",
    "\n",
    "# Query demonstrations\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"QUERY 1: All episodes for customer Geetha\")\n",
    "print(\"-\" * 60)\n",
    "geetha_episodes = episodic_store.query_by_customer(\"cust_7734891\")\n",
    "print(f\"Found {len(geetha_episodes)} episodes:\")\n",
    "for ep in geetha_episodes:\n",
    "    print(f\"\\n{ep.get_summary()}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"QUERY 2: All authentication issues\")\n",
    "print(\"-\" * 60)\n",
    "auth_episodes = episodic_store.query_by_category(\"authentication\")\n",
    "print(f\"Found {len(auth_episodes)} authentication episodes\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"STATISTICS:\")\n",
    "print(\"-\" * 60)\n",
    "stats = episodic_store.get_statistics()\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nâœ“ Episodic memory captures rich interaction details!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Semantic Memory - General Knowledge\n",
    "\n",
    "**Concept:** Semantic memory stores generalized knowledge extracted from multiple episodes.\n",
    "\n",
    "**SupportMax Pro Example:** OAuth timeout patterns learned from 847 similar episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SemanticKnowledge:\n",
    "    \"\"\"Represents generalized knowledge\"\"\"\n",
    "    knowledge_id: str\n",
    "    knowledge_type: str  # factual_rule, procedural_pattern, strategic_principle\n",
    "    domain: str\n",
    "    topic: str\n",
    "    knowledge_statement: str\n",
    "    confidence: float\n",
    "    source_episodes: int\n",
    "    success_rate: float\n",
    "    last_validated: datetime\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "    \n",
    "    def get_summary(self) -> str:\n",
    "        return f\"\"\"\n",
    "Knowledge ID: {self.knowledge_id}\n",
    "Type: {self.knowledge_type}\n",
    "Domain: {self.domain}\n",
    "Statement: {self.knowledge_statement}\n",
    "Confidence: {self.confidence:.2%}\n",
    "Based on: {self.source_episodes} episodes\n",
    "Success Rate: {self.success_rate:.2%}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResolutionPattern:\n",
    "    \"\"\"Learned pattern from multiple resolutions\"\"\"\n",
    "    pattern_id: str\n",
    "    pattern_name: str\n",
    "    success_instances: int\n",
    "    failure_instances: int\n",
    "    confidence_score: float\n",
    "    prerequisites: List[str]\n",
    "    diagnostic_steps: List[str]\n",
    "    resolution_steps: List[str]\n",
    "    average_time_to_resolve: float\n",
    "    customer_satisfaction_avg: float\n",
    "    \n",
    "    def get_success_rate(self) -> float:\n",
    "        total = self.success_instances + self.failure_instances\n",
    "        return self.success_instances / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "class SemanticMemoryStore:\n",
    "    \"\"\"Manages semantic knowledge\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.knowledge_base: List[SemanticKnowledge] = []\n",
    "        self.patterns: List[ResolutionPattern] = []\n",
    "    \n",
    "    def add_knowledge(self, knowledge: SemanticKnowledge):\n",
    "        \"\"\"Add new knowledge\"\"\"\n",
    "        self.knowledge_base.append(knowledge)\n",
    "    \n",
    "    def add_pattern(self, pattern: ResolutionPattern):\n",
    "        \"\"\"Add resolution pattern\"\"\"\n",
    "        self.patterns.append(pattern)\n",
    "    \n",
    "    def query_knowledge(self, domain: str) -> List[SemanticKnowledge]:\n",
    "        \"\"\"Get knowledge by domain\"\"\"\n",
    "        return [k for k in self.knowledge_base if k.domain == domain]\n",
    "    \n",
    "    def get_best_pattern(self, issue_category: str) -> Optional[ResolutionPattern]:\n",
    "        \"\"\"Get highest confidence pattern for issue\"\"\"\n",
    "        matching = [p for p in self.patterns if issue_category.lower() in p.pattern_name.lower()]\n",
    "        if not matching:\n",
    "            return None\n",
    "        return max(matching, key=lambda p: p.confidence_score)\n",
    "\n",
    "\n",
    "# Demo: Semantic Memory\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMONSTRATION: Semantic Memory System\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "semantic_store = SemanticMemoryStore()\n",
    "\n",
    "# Create semantic knowledge from episodic memories\n",
    "oauth_knowledge = SemanticKnowledge(\n",
    "    knowledge_id=\"KB-AUTH-0042\",\n",
    "    knowledge_type=\"factual_rule\",\n",
    "    domain=\"authentication\",\n",
    "    topic=\"OAuth 2.0 Configuration\",\n",
    "    knowledge_statement=\"OAuth timeouts between 25-35 seconds in version 2.8.4 \"\n",
    "                        \"indicate webhook URL protocol mismatch (http vs https)\",\n",
    "    confidence=0.94,\n",
    "    source_episodes=531,\n",
    "    success_rate=0.92,\n",
    "    last_validated=datetime.now(),\n",
    "    metadata={\n",
    "        \"software_versions\": [\"2.8.4\"],\n",
    "        \"common_causes\": [\"webhook_url_mismatch\", \"ssl_certificate_invalid\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create resolution pattern\n",
    "oauth_pattern = ResolutionPattern(\n",
    "    pattern_id=\"RP-AUTH-0089\",\n",
    "    pattern_name=\"OAuth Webhook URL Protocol Mismatch\",\n",
    "    success_instances=247,\n",
    "    failure_instances=12,\n",
    "    confidence_score=0.95,\n",
    "    prerequisites=[\n",
    "        \"customer_using_oauth\",\n",
    "        \"timeout_error_present\",\n",
    "        \"webhook_configured\"\n",
    "    ],\n",
    "    diagnostic_steps=[\n",
    "        \"Check OAuth configuration\",\n",
    "        \"Verify webhook URL protocol\",\n",
    "        \"Test SSL certificate validity\"\n",
    "    ],\n",
    "    resolution_steps=[\n",
    "        \"Update webhook URL to use https://\",\n",
    "        \"Verify SSL certificate is valid\",\n",
    "        \"Test OAuth flow end-to-end\",\n",
    "        \"Monitor for 24 hours\"\n",
    "    ],\n",
    "    average_time_to_resolve=15.2,\n",
    "    customer_satisfaction_avg=4.3\n",
    ")\n",
    "\n",
    "semantic_store.add_knowledge(oauth_knowledge)\n",
    "semantic_store.add_pattern(oauth_pattern)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"SEMANTIC KNOWLEDGE:\")\n",
    "print(\"-\" * 60)\n",
    "print(oauth_knowledge.get_summary())\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"RESOLUTION PATTERN:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Pattern: {oauth_pattern.pattern_name}\")\n",
    "print(f\"Success Rate: {oauth_pattern.get_success_rate():.2%}\")\n",
    "print(f\"Confidence: {oauth_pattern.confidence_score:.2%}\")\n",
    "print(f\"\\nResolution Steps:\")\n",
    "for i, step in enumerate(oauth_pattern.resolution_steps, 1):\n",
    "    print(f\"  {i}. {step}\")\n",
    "print(f\"\\nAverage Resolution Time: {oauth_pattern.average_time_to_resolve} minutes\")\n",
    "print(f\"Customer Satisfaction: {oauth_pattern.customer_satisfaction_avg}/5\")\n",
    "\n",
    "# Demonstrate using semantic memory for new issue\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"APPLYING SEMANTIC MEMORY TO NEW ISSUE:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nNew customer reports: 'OAuth timeout in version 2.8.4'\")\n",
    "print(\"\\nAgent queries semantic memory...\")\n",
    "\n",
    "best_pattern = semantic_store.get_best_pattern(\"OAuth\")\n",
    "if best_pattern:\n",
    "    print(f\"\\nâœ“ Found matching pattern: {best_pattern.pattern_name}\")\n",
    "    print(f\"  Confidence: {best_pattern.confidence_score:.2%}\")\n",
    "    print(f\"  Expected resolution time: {best_pattern.average_time_to_resolve} minutes\")\n",
    "    print(f\"\\n  Recommended steps:\")\n",
    "    for i, step in enumerate(best_pattern.resolution_steps, 1):\n",
    "        print(f\"    {i}. {step}\")\n",
    "\n",
    "print(\"\\nâœ“ Semantic memory enables instant application of learned patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Episode-to-Semantic Consolidation\n",
    "\n",
    "**Concept:** Transform individual episodes into generalized knowledge through pattern detection.\n",
    "\n",
    "**SupportMax Pro Example:** Nightly consolidation identifies recurring issues from daily tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryConsolidation:\n",
    "    \"\"\"Consolidates episodic memories into semantic knowledge\"\"\"\n",
    "    \n",
    "    def __init__(self, episodic_store: EpisodicMemoryStore, semantic_store: SemanticMemoryStore):\n",
    "        self.episodic_store = episodic_store\n",
    "        self.semantic_store = semantic_store\n",
    "    \n",
    "    def identify_high_impact_resolutions(self, days: int = 1) -> List[EpisodicMemory]:\n",
    "        \"\"\"Step 1: Find high-quality recent resolutions\"\"\"\n",
    "        recent = self.episodic_store.query_recent(days=days)\n",
    "        \n",
    "        # Filter for high quality\n",
    "        high_impact = [\n",
    "            ep for ep in recent\n",
    "            if ep.resolution_time_minutes < 30 and \n",
    "               ep.customer_satisfaction >= 4 and\n",
    "               not ep.escalated\n",
    "        ]\n",
    "        \n",
    "        return high_impact\n",
    "    \n",
    "    def extract_patterns(self, episodes: List[EpisodicMemory]) -> Dict[str, List[EpisodicMemory]]:\n",
    "        \"\"\"Step 2: Group episodes by pattern\"\"\"\n",
    "        patterns = defaultdict(list)\n",
    "        \n",
    "        for episode in episodes:\n",
    "            # Group by issue subcategory and resolution method\n",
    "            pattern_key = f\"{episode.issue_subcategory}_{episode.resolution_method}\"\n",
    "            patterns[pattern_key].append(episode)\n",
    "        \n",
    "        return dict(patterns)\n",
    "    \n",
    "    def detect_emerging_trends(self, patterns: Dict[str, List[EpisodicMemory]],\n",
    "                               threshold: int = 5) -> List[Tuple[str, List[EpisodicMemory]]]:\n",
    "        \"\"\"Step 3: Identify patterns appearing frequently\"\"\"\n",
    "        emerging = [\n",
    "            (pattern_name, episodes)\n",
    "            for pattern_name, episodes in patterns.items()\n",
    "            if len(episodes) >= threshold\n",
    "        ]\n",
    "        \n",
    "        # Sort by frequency\n",
    "        emerging.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "        \n",
    "        return emerging\n",
    "    \n",
    "    def create_semantic_knowledge(self, pattern_name: str, \n",
    "                                 episodes: List[EpisodicMemory]) -> SemanticKnowledge:\n",
    "        \"\"\"Step 4: Generate semantic knowledge from pattern\"\"\"\n",
    "        # Calculate statistics\n",
    "        avg_resolution_time = np.mean([ep.resolution_time_minutes for ep in episodes])\n",
    "        avg_satisfaction = np.mean([ep.customer_satisfaction for ep in episodes])\n",
    "        success_rate = sum(1 for ep in episodes if ep.customer_satisfaction >= 4) / len(episodes)\n",
    "        \n",
    "        # Extract common attributes\n",
    "        issue_category = episodes[0].issue_category\n",
    "        issue_subcategory = episodes[0].issue_subcategory\n",
    "        resolution_method = episodes[0].resolution_method\n",
    "        \n",
    "        # Create knowledge\n",
    "        knowledge = SemanticKnowledge(\n",
    "            knowledge_id=f\"KB-{issue_category.upper()}-{len(self.semantic_store.knowledge_base):04d}\",\n",
    "            knowledge_type=\"factual_rule\",\n",
    "            domain=issue_category,\n",
    "            topic=issue_subcategory,\n",
    "            knowledge_statement=f\"{issue_subcategory} issues typically resolve via {resolution_method} \"\n",
    "                                f\"in {avg_resolution_time:.1f} minutes with {success_rate:.0%} success rate\",\n",
    "            confidence=min(0.99, len(episodes) / 100),  # Higher confidence with more data\n",
    "            source_episodes=len(episodes),\n",
    "            success_rate=success_rate,\n",
    "            last_validated=datetime.now(),\n",
    "            metadata={\n",
    "                \"avg_resolution_time\": avg_resolution_time,\n",
    "                \"avg_satisfaction\": avg_satisfaction\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return knowledge\n",
    "    \n",
    "    def run_consolidation(self, days: int = 1) -> Dict:\n",
    "        \"\"\"Run complete consolidation process\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RUNNING MEMORY CONSOLIDATION (Last {days} day(s))\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Step 1\n",
    "        print(\"\\nStep 1: Identifying high-impact resolutions...\")\n",
    "        high_impact = self.identify_high_impact_resolutions(days=days)\n",
    "        print(f\"  âœ“ Found {len(high_impact)} high-impact resolutions\")\n",
    "        \n",
    "        # Step 2\n",
    "        print(\"\\nStep 2: Extracting patterns...\")\n",
    "        patterns = self.extract_patterns(high_impact)\n",
    "        print(f\"  âœ“ Identified {len(patterns)} unique patterns\")\n",
    "        \n",
    "        # Step 3\n",
    "        print(\"\\nStep 3: Detecting emerging trends...\")\n",
    "        emerging = self.detect_emerging_trends(patterns, threshold=2)\n",
    "        print(f\"  âœ“ Found {len(emerging)} emerging trends\")\n",
    "        \n",
    "        # Step 4\n",
    "        print(\"\\nStep 4: Creating semantic knowledge...\")\n",
    "        new_knowledge = []\n",
    "        for pattern_name, episodes in emerging:\n",
    "            knowledge = self.create_semantic_knowledge(pattern_name, episodes)\n",
    "            self.semantic_store.add_knowledge(knowledge)\n",
    "            new_knowledge.append(knowledge)\n",
    "            print(f\"  âœ“ Created knowledge: {knowledge.knowledge_id}\")\n",
    "        \n",
    "        return {\n",
    "            \"high_impact_count\": len(high_impact),\n",
    "            \"patterns_found\": len(patterns),\n",
    "            \"emerging_trends\": len(emerging),\n",
    "            \"new_knowledge_created\": len(new_knowledge),\n",
    "            \"knowledge\": new_knowledge\n",
    "        }\n",
    "\n",
    "\n",
    "# Demo: Memory Consolidation\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMONSTRATION: Episode-to-Semantic Consolidation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create more sample episodes for consolidation demo\n",
    "additional_episodes = [\n",
    "    EpisodicMemory(\n",
    "        episode_id=f\"EP-2024-{31550+i}\",\n",
    "        timestamp=datetime.now() - timedelta(hours=i),\n",
    "        customer_id=f\"cust_{9000+i}\",\n",
    "        customer_name=f\"Customer {i}\",\n",
    "        interaction_type=\"support_ticket\",\n",
    "        channel=\"email\",\n",
    "        issue_category=\"authentication\",\n",
    "        issue_subcategory=\"oauth_timeout\",\n",
    "        software_version=\"2.8.4\",\n",
    "        customer_tier=\"standard\" if i % 2 == 0 else \"premium\",\n",
    "        agent_id=f\"agent_technical_{i%3:03d}\",\n",
    "        conversation_turns=8 + i,\n",
    "        resolution_time_minutes=15 + i*2,\n",
    "        resolution_method=\"config_correction\",\n",
    "        resolution_details=\"Corrected webhook URL protocol\",\n",
    "        escalated=False,\n",
    "        customer_satisfaction=4 if i % 3 != 0 else 5,\n",
    "        tags=[\"oauth\", \"webhook\"],\n",
    "        related_episodes=[]\n",
    "    )\n",
    "    for i in range(5)\n",
    "]\n",
    "\n",
    "for ep in additional_episodes:\n",
    "    episodic_store.add_episode(ep)\n",
    "\n",
    "print(f\"\\nâœ“ Added {len(additional_episodes)} more episodes for consolidation demo\")\n",
    "\n",
    "# Run consolidation\n",
    "consolidator = MemoryConsolidation(episodic_store, semantic_store)\n",
    "results = consolidator.run_consolidation(days=7)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"CONSOLIDATION RESULTS:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"High-impact resolutions found: {results['high_impact_count']}\")\n",
    "print(f\"Unique patterns identified: {results['patterns_found']}\")\n",
    "print(f\"Emerging trends detected: {results['emerging_trends']}\")\n",
    "print(f\"New knowledge created: {results['new_knowledge_created']}\")\n",
    "\n",
    "# Show created knowledge\n",
    "if results['knowledge']:\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"NEW SEMANTIC KNOWLEDGE:\")\n",
    "    print(\"-\" * 60)\n",
    "    for knowledge in results['knowledge']:\n",
    "        print(f\"\\n{knowledge.get_summary()}\")\n",
    "\n",
    "print(\"\\nâœ“ Consolidation transforms specific experiences into general knowledge!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Vector Database Implementation\n",
    "\n",
    "### 4.1 Embedding-based Semantic Search\n",
    "\n",
    "**Concept:** Store ticket descriptions as embeddings for similarity-based retrieval.\n",
    "\n",
    "**SupportMax Pro Example:** Find similar past issues using vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store with Chroma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class VectorMemoryStore:\n",
    "    \"\"\"Vector-based memory using embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Use sentence transformers for embeddings\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_ticket(self, ticket_id: str, description: str, metadata: Dict):\n",
    "        \"\"\"Add ticket to vector store\"\"\"\n",
    "        # Generate embedding\n",
    "        embedding = self.embedding_model.encode(description)\n",
    "        \n",
    "        self.documents.append(description)\n",
    "        self.embeddings.append(embedding)\n",
    "        self.metadata.append({**metadata, \"ticket_id\": ticket_id})\n",
    "    \n",
    "    def similarity_search(self, query: str, top_k: int = 3) -> List[Tuple[str, float, Dict]]:\n",
    "        \"\"\"Find similar tickets using cosine similarity\"\"\"\n",
    "        # Encode query\n",
    "        query_embedding = self.embedding_model.encode(query)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = []\n",
    "        for i, doc_embedding in enumerate(self.embeddings):\n",
    "            similarity = np.dot(query_embedding, doc_embedding) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)\n",
    "            )\n",
    "            similarities.append((self.documents[i], similarity, self.metadata[i]))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get vector store statistics\"\"\"\n",
    "        return {\n",
    "            \"total_documents\": len(self.documents),\n",
    "            \"embedding_dimension\": len(self.embeddings[0]) if self.embeddings else 0,\n",
    "            \"total_size_mb\": (len(self.embeddings) * len(self.embeddings[0]) * 4) / (1024 * 1024) if self.embeddings else 0\n",
    "        }\n",
    "\n",
    "\n",
    "# Demo: Vector Memory Store\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMONSTRATION: Vector-based Semantic Search\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "vector_store = VectorMemoryStore()\n",
    "\n",
    "# Add sample tickets\n",
    "sample_tickets = [\n",
    "    {\n",
    "        \"ticket_id\": \"TKT-001\",\n",
    "        \"description\": \"OAuth authentication timeout when connecting to API. Getting 30 second timeout errors.\",\n",
    "        \"metadata\": {\"category\": \"authentication\", \"version\": \"2.8.4\", \"resolution\": \"webhook_fix\"}\n",
    "    },\n",
    "    {\n",
    "        \"ticket_id\": \"TKT-002\",\n",
    "        \"description\": \"SSL certificate validation failing during OAuth handshake. HTTPS connection errors.\",\n",
    "        \"metadata\": {\"category\": \"authentication\", \"version\": \"2.8.4\", \"resolution\": \"certificate_renewal\"}\n",
    "    },\n",
    "    {\n",
    "        \"ticket_id\": \"TKT-003\",\n",
    "        \"description\": \"API rate limit exceeded. Getting 429 errors when making bulk requests.\",\n",
    "        \"metadata\": {\"category\": \"api\", \"version\": \"3.0.0\", \"resolution\": \"rate_limit_increase\"}\n",
    "    },\n",
    "    {\n",
    "        \"ticket_id\": \"TKT-004\",\n",
    "        \"description\": \"Billing invoice shows incorrect charges. Double billing for same period.\",\n",
    "        \"metadata\": {\"category\": \"billing\", \"version\": \"N/A\", \"resolution\": \"invoice_correction\"}\n",
    "    },\n",
    "    {\n",
    "        \"ticket_id\": \"TKT-005\",\n",
    "        \"description\": \"Authentication webhook not receiving callbacks. Timeout after 35 seconds.\",\n",
    "        \"metadata\": {\"category\": \"authentication\", \"version\": \"2.8.4\", \"resolution\": \"webhook_fix\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nAdding tickets to vector store...\")\n",
    "for ticket in sample_tickets:\n",
    "    vector_store.add_ticket(\n",
    "        ticket[\"ticket_id\"],\n",
    "        ticket[\"description\"],\n",
    "        ticket[\"metadata\"]\n",
    "    )\n",
    "    print(f\"  âœ“ Added {ticket['ticket_id']}\")\n",
    "\n",
    "# Display stats\n",
    "stats = vector_store.get_stats()\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"VECTOR STORE STATISTICS:\")\n",
    "print(\"-\" * 60)\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Semantic search demo\n",
    "queries = [\n",
    "    \"Customer getting OAuth timeout issues\",\n",
    "    \"Invoice showing wrong amount\",\n",
    "    \"Too many API requests being blocked\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(f\"QUERY: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = vector_store.similarity_search(query, top_k=3)\n",
    "    \n",
    "    print(f\"\\nTop {len(results)} similar tickets:\")\n",
    "    for i, (doc, similarity, metadata) in enumerate(results, 1):\n",
    "        print(f\"\\n  {i}. {metadata['ticket_id']} (similarity: {similarity:.3f})\")\n",
    "        print(f\"     Description: {doc}\")\n",
    "        print(f\"     Category: {metadata['category']}\")\n",
    "        print(f\"     Resolution: {metadata['resolution']}\")\n",
    "\n",
    "print(\"\\nâœ“ Vector search finds semantically similar issues, not just keyword matches!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Modern Memory Frameworks - Mem0 Integration\n",
    "\n",
    "### 5.1 Mem0 - The Memory Layer for AI Agents\n",
    "\n",
    "**Concept:** Mem0 provides intelligent memory management with automatic storage, retrieval, and conflict resolution.\n",
    "\n",
    "**SupportMax Pro Example:** Persistent customer context across multiple support sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Mem0-like implementation (since actual Mem0 requires API setup)\n",
    "class SimplifiedMem0:\n",
    "    \"\"\"Simplified memory system inspired by Mem0\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.memories = {}  # user_id -> List[memory]\n",
    "        self.embeddings = {}  # user_id -> List[embeddings]\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    def add(self, messages: List[Dict], user_id: str, metadata: Dict = None):\n",
    "        \"\"\"Add memories from conversation\"\"\"\n",
    "        # Extract important information\n",
    "        memory_text = \" \".join([msg[\"content\"] for msg in messages])\n",
    "        \n",
    "        # Create memory entry\n",
    "        memory_entry = {\n",
    "            \"id\": f\"mem_{user_id}_{len(self.memories.get(user_id, []))}\",\n",
    "            \"text\": memory_text,\n",
    "            \"metadata\": metadata or {},\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"user_id\": user_id\n",
    "        }\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = self.embedding_model.encode(memory_text)\n",
    "        \n",
    "        # Store\n",
    "        if user_id not in self.memories:\n",
    "            self.memories[user_id] = []\n",
    "            self.embeddings[user_id] = []\n",
    "        \n",
    "        self.memories[user_id].append(memory_entry)\n",
    "        self.embeddings[user_id].append(embedding)\n",
    "        \n",
    "        return memory_entry[\"id\"]\n",
    "    \n",
    "    def get_all(self, user_id: str) -> List[Dict]:\n",
    "        \"\"\"Retrieve all memories for user\"\"\"\n",
    "        return self.memories.get(user_id, [])\n",
    "    \n",
    "    def search(self, query: str, user_id: str, limit: int = 3) -> List[Dict]:\n",
    "        \"\"\"Search memories semantically\"\"\"\n",
    "        if user_id not in self.memories:\n",
    "            return []\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = self.embedding_model.encode(query)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        results = []\n",
    "        for memory, embedding in zip(self.memories[user_id], self.embeddings[user_id]):\n",
    "            similarity = np.dot(query_embedding, embedding) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(embedding)\n",
    "            )\n",
    "            results.append((memory, similarity))\n",
    "        \n",
    "        # Sort and return top results\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [r[0] for r in results[:limit]]\n",
    "    \n",
    "    def update(self, memory_id: str, data: Dict):\n",
    "        \"\"\"Update existing memory\"\"\"\n",
    "        # Find and update memory\n",
    "        for user_id, memories in self.memories.items():\n",
    "            for i, memory in enumerate(memories):\n",
    "                if memory[\"id\"] == memory_id:\n",
    "                    memories[i].update(data)\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def delete(self, memory_id: str):\n",
    "        \"\"\"Delete memory\"\"\"\n",
    "        for user_id, memories in self.memories.items():\n",
    "            for i, memory in enumerate(memories):\n",
    "                if memory[\"id\"] == memory_id:\n",
    "                    del self.memories[user_id][i]\n",
    "                    del self.embeddings[user_id][i]\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "\n",
    "# Demo: Mem0-style Memory\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMONSTRATION: Mem0-style Intelligent Memory\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mem0 = SimplifiedMem0()\n",
    "\n",
    "# Simulate customer interactions over time\n",
    "customer_id = \"cust_7734891\"\n",
    "\n",
    "# Session 1: Initial OAuth issue\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"SESSION 1: Initial OAuth Issue\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "session1_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"I'm getting OAuth timeout errors in version 2.8.4\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'll help resolve the OAuth timeout. Let me check your configuration.\"},\n",
    "    {\"role\": \"user\", \"content\": \"The error happens when authenticating with our webhook\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Fixed by updating webhook URL to use HTTPS protocol\"}\n",
    "]\n",
    "\n",
    "mem_id1 = mem0.add(\n",
    "    session1_messages,\n",
    "    user_id=customer_id,\n",
    "    metadata={\n",
    "        \"ticket_id\": \"TKT-001\",\n",
    "        \"issue\": \"oauth_timeout\",\n",
    "        \"resolution\": \"webhook_https_fix\",\n",
    "        \"satisfaction\": 4\n",
    "    }\n",
    ")\n",
    "print(f\"âœ“ Stored memory: {mem_id1}\")\n",
    "\n",
    "# Session 2: Follow-up issue (7 days later)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"SESSION 2: Follow-up Issue (7 days later)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "session2_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"The OAuth timeout is happening again\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I see we fixed this before. Let me check if it's a recurring issue.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Yes, it worked for a week but now it's back\"},\n",
    "]\n",
    "\n",
    "# Search previous memories\n",
    "print(\"\\nAgent searches memory for similar issues...\")\n",
    "relevant_memories = mem0.search(\"OAuth timeout\", user_id=customer_id)\n",
    "\n",
    "if relevant_memories:\n",
    "    print(f\"\\nâœ“ Found {len(relevant_memories)} relevant previous interactions:\")\n",
    "    for i, memory in enumerate(relevant_memories, 1):\n",
    "        print(f\"\\n  Memory {i}:\")\n",
    "        print(f\"    Ticket: {memory['metadata'].get('ticket_id', 'N/A')}\")\n",
    "        print(f\"    Issue: {memory['metadata'].get('issue', 'N/A')}\")\n",
    "        print(f\"    Resolution: {memory['metadata'].get('resolution', 'N/A')}\")\n",
    "        print(f\"    Date: {memory['timestamp'].strftime('%Y-%m-%d %H:%M')}\")\n",
    "\n",
    "session2_messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Since this is recurring, I'll escalate for deeper investigation of SSL certificates\"\n",
    "})\n",
    "\n",
    "mem_id2 = mem0.add(\n",
    "    session2_messages,\n",
    "    user_id=customer_id,\n",
    "    metadata={\n",
    "        \"ticket_id\": \"TKT-002\",\n",
    "        \"issue\": \"oauth_timeout_recurring\",\n",
    "        \"resolution\": \"escalated_ssl_investigation\",\n",
    "        \"satisfaction\": 3,\n",
    "        \"related_to\": mem_id1\n",
    "    }\n",
    ")\n",
    "print(f\"\\nâœ“ Stored memory: {mem_id2}\")\n",
    "\n",
    "# Session 3: Different customer, similar issue\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"SESSION 3: Different Customer, Similar Issue\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "customer2_id = \"cust_8845002\"\n",
    "session3_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Getting timeout when authenticating via OAuth\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Based on similar cases, let me check your webhook protocol\"},\n",
    "]\n",
    "\n",
    "# Agent can reference patterns from other customers (semantic memory)\n",
    "print(\"\\nAgent applies learned pattern from previous customer...\")\n",
    "print(\"  âœ“ Identified webhook HTTPS as common solution\")\n",
    "print(\"  âœ“ Resolved in 10 minutes (vs 23 minutes for first customer)\")\n",
    "\n",
    "mem_id3 = mem0.add(\n",
    "    session3_messages,\n",
    "    user_id=customer2_id,\n",
    "    metadata={\n",
    "        \"ticket_id\": \"TKT-003\",\n",
    "        \"issue\": \"oauth_timeout\",\n",
    "        \"resolution\": \"webhook_https_fix\",\n",
    "        \"satisfaction\": 5,\n",
    "        \"resolution_time\": 10\n",
    "    }\n",
    ")\n",
    "print(f\"\\nâœ“ Stored memory: {mem_id3}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MEMORY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_customers = [customer_id, customer2_id]\n",
    "for cust_id in all_customers:\n",
    "    memories = mem0.get_all(cust_id)\n",
    "    print(f\"\\nCustomer {cust_id}: {len(memories)} interaction(s)\")\n",
    "    for memory in memories:\n",
    "        print(f\"  - {memory['metadata'].get('ticket_id')}: \"\n",
    "              f\"{memory['metadata'].get('issue')} \"\n",
    "              f\"(satisfaction: {memory['metadata'].get('satisfaction')}/5)\")\n",
    "\n",
    "print(\"\\nâœ“ Mem0-style memory enables:\")\n",
    "print(\"  â€¢ Automatic context retention across sessions\")\n",
    "print(\"  â€¢ Semantic search for relevant past interactions\")\n",
    "print(\"  â€¢ Pattern learning across customers\")\n",
    "print(\"  â€¢ Faster resolution through learned knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Distributed State Management\n",
    "\n",
    "### 6.1 Managing State Across Multiple Agent Instances\n",
    "\n",
    "**Concept:** Coordinate shared state when multiple agents handle concurrent tickets.\n",
    "\n",
    "**SupportMax Pro Example:** 150 concurrent agents need consistent customer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "\n",
    "class DistributedStateManager:\n",
    "    \"\"\"Manages state across distributed agents\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Simulated Redis-like cache\n",
    "        self.working_memory = {}  # ticket_id -> state\n",
    "        self.locks = {}  # ticket_id -> lock\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def acquire_lock(self, ticket_id: str, agent_id: str, timeout: float = 5.0) -> bool:\n",
    "        \"\"\"Acquire lock for ticket processing\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < timeout:\n",
    "            with self.lock:\n",
    "                if ticket_id not in self.locks or self.locks[ticket_id] is None:\n",
    "                    self.locks[ticket_id] = {\n",
    "                        \"agent_id\": agent_id,\n",
    "                        \"acquired_at\": time.time()\n",
    "                    }\n",
    "                    return True\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def release_lock(self, ticket_id: str, agent_id: str) -> bool:\n",
    "        \"\"\"Release lock on ticket\"\"\"\n",
    "        with self.lock:\n",
    "            if ticket_id in self.locks and self.locks[ticket_id][\"agent_id\"] == agent_id:\n",
    "                self.locks[ticket_id] = None\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_state(self, ticket_id: str) -> Optional[Dict]:\n",
    "        \"\"\"Get current ticket state\"\"\"\n",
    "        return self.working_memory.get(ticket_id)\n",
    "    \n",
    "    def set_state(self, ticket_id: str, state: Dict, agent_id: str) -> bool:\n",
    "        \"\"\"Update ticket state (requires lock)\"\"\"\n",
    "        with self.lock:\n",
    "            if ticket_id in self.locks and self.locks[ticket_id][\"agent_id\"] == agent_id:\n",
    "                self.working_memory[ticket_id] = state\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get state manager statistics\"\"\"\n",
    "        active_locks = sum(1 for lock in self.locks.values() if lock is not None)\n",
    "        return {\n",
    "            \"total_tickets\": len(self.working_memory),\n",
    "            \"active_locks\": active_locks,\n",
    "            \"total_locks\": len(self.locks)\n",
    "        }\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Simulated agent that processes tickets\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_id: str, state_manager: DistributedStateManager):\n",
    "        self.agent_id = agent_id\n",
    "        self.state_manager = state_manager\n",
    "        self.processed_tickets = []\n",
    "    \n",
    "    def process_ticket(self, ticket_id: str) -> bool:\n",
    "        \"\"\"Process a ticket with distributed locking\"\"\"\n",
    "        print(f\"\\n[{self.agent_id}] Attempting to process {ticket_id}...\")\n",
    "        \n",
    "        # Try to acquire lock\n",
    "        if not self.state_manager.acquire_lock(ticket_id, self.agent_id):\n",
    "            print(f\"[{self.agent_id}] âœ— Could not acquire lock for {ticket_id}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"[{self.agent_id}] âœ“ Acquired lock for {ticket_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Get current state\n",
    "            state = self.state_manager.get_state(ticket_id)\n",
    "            if state is None:\n",
    "                state = {\"status\": \"new\", \"updates\": []}\n",
    "            \n",
    "            # Simulate processing\n",
    "            time.sleep(0.5)  # Simulate work\n",
    "            \n",
    "            # Update state\n",
    "            state[\"status\"] = \"in_progress\"\n",
    "            state[\"updates\"].append({\n",
    "                \"agent_id\": self.agent_id,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"action\": \"processed\"\n",
    "            })\n",
    "            \n",
    "            # Save state\n",
    "            if self.state_manager.set_state(ticket_id, state, self.agent_id):\n",
    "                print(f\"[{self.agent_id}] âœ“ Updated state for {ticket_id}\")\n",
    "                self.processed_tickets.append(ticket_id)\n",
    "            else:\n",
    "                print(f\"[{self.agent_id}] âœ— Failed to update state for {ticket_id}\")\n",
    "                return False\n",
    "            \n",
    "        finally:\n",
    "            # Always release lock\n",
    "            self.state_manager.release_lock(ticket_id, self.agent_id)\n",
    "            print(f\"[{self.agent_id}] âœ“ Released lock for {ticket_id}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "\n",
    "# Demo: Distributed State Management\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMONSTRATION: Distributed State Management\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "state_manager = DistributedStateManager()\n",
    "\n",
    "# Create multiple agents\n",
    "agents = [\n",
    "    Agent(\"agent_001\", state_manager),\n",
    "    Agent(\"agent_002\", state_manager),\n",
    "    Agent(\"agent_003\", state_manager)\n",
    "]\n",
    "\n",
    "# Simulate concurrent ticket processing\n",
    "tickets = [\"TKT-001\", \"TKT-002\", \"TKT-003\", \"TKT-004\"]\n",
    "\n",
    "print(\"\\nSimulating concurrent ticket processing...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create threads for concurrent processing\n",
    "threads = []\n",
    "for i, ticket_id in enumerate(tickets):\n",
    "    agent = agents[i % len(agents)]\n",
    "    thread = threading.Thread(target=agent.process_ticket, args=(ticket_id,))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "# Wait for all to complete\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "# Show results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROCESSING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for agent in agents:\n",
    "    print(f\"\\n{agent.agent_id}: Processed {len(agent.processed_tickets)} ticket(s)\")\n",
    "    for ticket in agent.processed_tickets:\n",
    "        print(f\"  - {ticket}\")\n",
    "\n",
    "# Show final state\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"FINAL STATE:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "stats = state_manager.get_stats()\n",
    "print(f\"Total tickets in state: {stats['total_tickets']}\")\n",
    "print(f\"Active locks: {stats['active_locks']}\")\n",
    "\n",
    "print(\"\\nâœ“ Distributed locks prevent race conditions and ensure consistency!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Memory Performance Metrics\n",
    "\n",
    "### 7.1 Measuring Memory System Performance\n",
    "\n",
    "**Concept:** Track key metrics like latency, hit rates, and consolidation effectiveness.\n",
    "\n",
    "**SupportMax Pro Example:** Monitor memory system to meet 2-minute SLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryMetrics:\n",
    "    \"\"\"Track memory system performance metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.access_times = []\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        self.consolidation_runs = []\n",
    "        self.semantic_search_times = []\n",
    "    \n",
    "    def record_access(self, latency_ms: float, cache_hit: bool):\n",
    "        \"\"\"Record memory access\"\"\"\n",
    "        self.access_times.append(latency_ms)\n",
    "        if cache_hit:\n",
    "            self.cache_hits += 1\n",
    "        else:\n",
    "            self.cache_misses += 1\n",
    "    \n",
    "    def record_consolidation(self, episodes_processed: int, patterns_created: int, duration_sec: float):\n",
    "        \"\"\"Record consolidation run\"\"\"\n",
    "        self.consolidation_runs.append({\n",
    "            \"episodes\": episodes_processed,\n",
    "            \"patterns\": patterns_created,\n",
    "            \"duration\": duration_sec,\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "    \n",
    "    def record_semantic_search(self, latency_ms: float):\n",
    "        \"\"\"Record semantic search operation\"\"\"\n",
    "        self.semantic_search_times.append(latency_ms)\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get performance summary\"\"\"\n",
    "        total_accesses = self.cache_hits + self.cache_misses\n",
    "        cache_hit_rate = self.cache_hits / total_accesses if total_accesses > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"memory_access\": {\n",
    "                \"total_accesses\": total_accesses,\n",
    "                \"cache_hit_rate\": cache_hit_rate,\n",
    "                \"avg_latency_ms\": np.mean(self.access_times) if self.access_times else 0,\n",
    "                \"p50_latency_ms\": np.percentile(self.access_times, 50) if self.access_times else 0,\n",
    "                \"p95_latency_ms\": np.percentile(self.access_times, 95) if self.access_times else 0,\n",
    "                \"p99_latency_ms\": np.percentile(self.access_times, 99) if self.access_times else 0\n",
    "            },\n",
    "            \"consolidation\": {\n",
    "                \"total_runs\": len(self.consolidation_runs),\n",
    "                \"total_episodes\": sum(r[\"episodes\"] for r in self.consolidation_runs),\n",
    "                \"total_patterns\": sum(r[\"patterns\"] for r in self.consolidation_runs),\n",
    "                \"avg_duration_sec\": np.mean([r[\"duration\"] for r in self.consolidation_runs]) if self.consolidation_runs else 0\n",
    "            },\n",
    "            \"semantic_search\": {\n",
    "                \"total_searches\": len(self.semantic_search_times),\n",
    "                \"avg_latency_ms\": np.mean(self.semantic_search_times) if self.semantic_search_times else 0,\n",
    "                \"p95_latency_ms\": np.percentile(self.semantic_search_times, 95) if self.semantic_search_times else 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Visualize performance metrics\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Access latency distribution\n",
    "        if self.access_times:\n",
    "            axes[0, 0].hist(self.access_times, bins=30, edgecolor='black')\n",
    "            axes[0, 0].set_title('Memory Access Latency Distribution')\n",
    "            axes[0, 0].set_xlabel('Latency (ms)')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            axes[0, 0].axvline(np.percentile(self.access_times, 95), color='red', \n",
    "                              linestyle='--', label='P95')\n",
    "            axes[0, 0].legend()\n",
    "        \n",
    "        # Cache hit rate\n",
    "        cache_data = [self.cache_hits, self.cache_misses]\n",
    "        axes[0, 1].pie(cache_data, labels=['Cache Hits', 'Cache Misses'], \n",
    "                       autopct='%1.1f%%', startangle=90)\n",
    "        axes[0, 1].set_title('Cache Hit Rate')\n",
    "        \n",
    "        # Consolidation trends\n",
    "        if self.consolidation_runs:\n",
    "            runs = list(range(1, len(self.consolidation_runs) + 1))\n",
    "            patterns = [r[\"patterns\"] for r in self.consolidation_runs]\n",
    "            axes[1, 0].plot(runs, patterns, marker='o')\n",
    "            axes[1, 0].set_title('Patterns Created per Consolidation Run')\n",
    "            axes[1, 0].set_xlabel('Run Number')\n",
    "            axes[1, 0].set_ylabel('Patterns Created')\n",
    "            axes[1, 0].grid(True)\n",
    "        \n",
    "        # Semantic search latency\n",
    "        if self.semantic_search_times:\n",
    "            axes[1, 1].boxplot(self.semantic_search_times)\n",
    "            axes[1, 1].set_title('Semantic Search Latency')\n",
    "            axes[1, 1].set_ylabel('Latency (ms)')\n",
    "            axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Demo: Memory Metrics\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMONSTRATION: Memory Performance Metrics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "metrics = MemoryMetrics()\n",
    "\n",
    "# Simulate various operations\n",
    "print(\"\\nSimulating memory operations...\")\n",
    "\n",
    "# Memory accesses (80% cache hits)\n",
    "np.random.seed(42)\n",
    "for _ in range(1000):\n",
    "    cache_hit = np.random.random() < 0.80\n",
    "    latency = np.random.normal(8, 2) if cache_hit else np.random.normal(50, 10)\n",
    "    metrics.record_access(max(1, latency), cache_hit)\n",
    "\n",
    "# Consolidation runs\n",
    "for i in range(7):  # Daily for a week\n",
    "    episodes = np.random.randint(100, 200)\n",
    "    patterns = np.random.randint(5, 15)\n",
    "    duration = np.random.uniform(30, 90)\n",
    "    metrics.record_consolidation(episodes, patterns, duration)\n",
    "\n",
    "# Semantic searches\n",
    "for _ in range(200):\n",
    "    latency = np.random.normal(45, 12)\n",
    "    metrics.record_semantic_search(max(10, latency))\n",
    "\n",
    "# Get summary\n",
    "summary = metrics.get_summary()\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"MEMORY ACCESS METRICS:\")\n",
    "print(\"-\" * 60)\n",
    "for key, value in summary[\"memory_access\"].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"CONSOLIDATION METRICS:\")\n",
    "print(\"-\" * 60)\n",
    "for key, value in summary[\"consolidation\"].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"SEMANTIC SEARCH METRICS:\")\n",
    "print(\"-\" * 60)\n",
    "for key, value in summary[\"semantic_search\"].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Assess SLA compliance\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SLA COMPLIANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "p95_latency = summary[\"memory_access\"][\"p95_latency_ms\"]\n",
    "sla_target = 100  # 100ms for memory operations\n",
    "\n",
    "print(f\"\\nP95 Memory Access Latency: {p95_latency:.2f}ms\")\n",
    "print(f\"SLA Target: {sla_target}ms\")\n",
    "\n",
    "if p95_latency < sla_target:\n",
    "    print(f\"âœ“ PASSING - {((sla_target - p95_latency) / sla_target * 100):.1f}% headroom\")\n",
    "else:\n",
    "    print(f\"âœ— FAILING - {((p95_latency - sla_target) / sla_target * 100):.1f}% over target\")\n",
    "\n",
    "# Plot metrics\n",
    "print(\"\\nGenerating visualization...\")\n",
    "metrics.plot_metrics()\n",
    "\n",
    "print(\"\\nâœ“ Comprehensive metrics enable performance optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Complete SupportMax Pro Memory Architecture\n",
    "\n",
    "### 8.1 Integrated Memory System\n",
    "\n",
    "**Concept:** Combine all memory layers into production-ready architecture.\n",
    "\n",
    "**SupportMax Pro Example:** Three-tier memory with Redis, PostgreSQL, and Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportMaxProMemoryArchitecture:\n",
    "    \"\"\"Complete memory system for SupportMax Pro\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Layer 1: Working Memory (Redis simulation)\n",
    "        self.working_memory = ImmediateWorkingMemory()\n",
    "        \n",
    "        # Layer 2: Operational Memory (PostgreSQL simulation)\n",
    "        self.episodic_store = EpisodicMemoryStore()\n",
    "        self.session_store = {}  # session_id -> SessionMemory\n",
    "        \n",
    "        # Layer 3: Strategic Memory (Vector DB simulation)\n",
    "        self.semantic_store = SemanticMemoryStore()\n",
    "        self.vector_store = VectorMemoryStore()\n",
    "        \n",
    "        # Supporting systems\n",
    "        self.state_manager = DistributedStateManager()\n",
    "        self.metrics = MemoryMetrics()\n",
    "        self.consolidator = MemoryConsolidation(self.episodic_store, self.semantic_store)\n",
    "    \n",
    "    def handle_customer_message(self, customer_id: str, ticket_id: str, \n",
    "                               message: str, agent_id: str) -> Dict:\n",
    "        \"\"\"Process customer message using full memory architecture\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. Check working memory (Layer 1 - fastest)\n",
    "        self.working_memory.add_message(message, \"user\")\n",
    "        current_context = self.working_memory.get_context()\n",
    "        \n",
    "        # 2. Retrieve episodic memory (Layer 2)\n",
    "        customer_history = self.episodic_store.query_by_customer(customer_id)\n",
    "        \n",
    "        # 3. Search semantic memory (Layer 3)\n",
    "        similar_tickets = self.vector_store.similarity_search(message, top_k=3)\n",
    "        relevant_knowledge = self.semantic_store.query_knowledge(\"authentication\")\n",
    "        \n",
    "        # Record metrics\n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        self.metrics.record_access(latency, cache_hit=len(customer_history) > 0)\n",
    "        \n",
    "        return {\n",
    "            \"current_context\": current_context,\n",
    "            \"customer_history_count\": len(customer_history),\n",
    "            \"similar_tickets\": len(similar_tickets),\n",
    "            \"relevant_knowledge\": len(relevant_knowledge),\n",
    "            \"latency_ms\": latency\n",
    "        }\n",
    "    \n",
    "    def resolve_ticket(self, episode: EpisodicMemory):\n",
    "        \"\"\"Record resolved ticket across all memory layers\"\"\"\n",
    "        # Store in episodic memory\n",
    "        self.episodic_store.add_episode(episode)\n",
    "        \n",
    "        # Add to vector store\n",
    "        description = f\"{episode.issue_subcategory}: {episode.resolution_details}\"\n",
    "        self.vector_store.add_ticket(\n",
    "            episode.episode_id,\n",
    "            description,\n",
    "            {\n",
    "                \"category\": episode.issue_category,\n",
    "                \"resolution\": episode.resolution_method,\n",
    "                \"satisfaction\": episode.customer_satisfaction\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def run_nightly_consolidation(self):\n",
    "        \"\"\"Run memory consolidation\"\"\"\n",
    "        start_time = time.time()\n",
    "        results = self.consolidator.run_consolidation(days=1)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        self.metrics.record_consolidation(\n",
    "            results[\"high_impact_count\"],\n",
    "            results[\"new_knowledge_created\"],\n",
    "            duration\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_system_status(self) -> Dict:\n",
    "        \"\"\"Get comprehensive system status\"\"\"\n",
    "        return {\n",
    "            \"working_memory\": {\n",
    "                \"context_length\": len(self.working_memory.context_window)\n",
    "            },\n",
    "            \"episodic_memory\": self.episodic_store.get_statistics(),\n",
    "            \"semantic_memory\": {\n",
    "                \"knowledge_count\": len(self.semantic_store.knowledge_base),\n",
    "                \"pattern_count\": len(self.semantic_store.patterns)\n",
    "            },\n",
    "            \"vector_store\": self.vector_store.get_stats(),\n",
    "            \"state_manager\": self.state_manager.get_stats(),\n",
    "            \"performance\": self.metrics.get_summary()\n",
    "        }\n",
    "\n",
    "\n",
    "# Demo: Complete Architecture\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMONSTRATION: Complete SupportMax Pro Memory Architecture\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize system\n",
    "print(\"\\nInitializing SupportMax Pro memory system...\")\n",
    "supportmax = SupportMaxProMemoryArchitecture()\n",
    "print(\"âœ“ System initialized\")\n",
    "\n",
    "# Simulate day of operations\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"SIMULATING DAY 1: Initial Tickets\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Process multiple customer interactions\n",
    "interactions = [\n",
    "    {\n",
    "        \"customer_id\": \"cust_7734891\",\n",
    "        \"ticket_id\": \"TKT-001\",\n",
    "        \"message\": \"OAuth timeout in version 2.8.4\",\n",
    "        \"agent_id\": \"agent_001\"\n",
    "    },\n",
    "    {\n",
    "        \"customer_id\": \"cust_8845002\",\n",
    "        \"ticket_id\": \"TKT-002\",\n",
    "        \"message\": \"Getting SSL errors during authentication\",\n",
    "        \"agent_id\": \"agent_002\"\n",
    "    },\n",
    "    {\n",
    "        \"customer_id\": \"cust_9956113\",\n",
    "        \"ticket_id\": \"TKT-003\",\n",
    "        \"message\": \"API timeout when calling webhook\",\n",
    "        \"agent_id\": \"agent_003\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for interaction in interactions:\n",
    "    result = supportmax.handle_customer_message(**interaction)\n",
    "    print(f\"\\n{interaction['ticket_id']}:\")\n",
    "    print(f\"  Latency: {result['latency_ms']:.2f}ms\")\n",
    "    print(f\"  Similar tickets found: {result['similar_tickets']}\")\n",
    "\n",
    "# Resolve tickets\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"RESOLVING TICKETS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "resolved_episodes = [\n",
    "    EpisodicMemory(\n",
    "        episode_id=\"EP-2024-001\",\n",
    "        timestamp=datetime.now(),\n",
    "        customer_id=\"cust_7734891\",\n",
    "        customer_name=\"Geetha\",\n",
    "        interaction_type=\"support_ticket\",\n",
    "        channel=\"email\",\n",
    "        issue_category=\"authentication\",\n",
    "        issue_subcategory=\"oauth_timeout\",\n",
    "        software_version=\"2.8.4\",\n",
    "        customer_tier=\"premium\",\n",
    "        agent_id=\"agent_001\",\n",
    "        conversation_turns=8,\n",
    "        resolution_time_minutes=18,\n",
    "        resolution_method=\"webhook_https_fix\",\n",
    "        resolution_details=\"Updated webhook URL to HTTPS\",\n",
    "        escalated=False,\n",
    "        customer_satisfaction=5,\n",
    "        tags=[\"oauth\", \"webhook\"],\n",
    "        related_episodes=[]\n",
    "    )\n",
    "]\n",
    "\n",
    "for episode in resolved_episodes:\n",
    "    supportmax.resolve_ticket(episode)\n",
    "    print(f\"âœ“ Resolved {episode.episode_id}\")\n",
    "\n",
    "# Run consolidation\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"NIGHTLY CONSOLIDATION\")\n",
    "print(\"-\" * 60)\n",
    "consolidation_results = supportmax.run_nightly_consolidation()\n",
    "\n",
    "# Show system status\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SYSTEM STATUS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "status = supportmax.get_system_status()\n",
    "\n",
    "print(\"\\nWorking Memory:\")\n",
    "print(f\"  Context window size: {status['working_memory']['context_length']} messages\")\n",
    "\n",
    "print(\"\\nEpisodic Memory:\")\n",
    "for key, value in status['episodic_memory'].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nSemantic Memory:\")\n",
    "for key, value in status['semantic_memory'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nVector Store:\")\n",
    "for key, value in status['vector_store'].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ“ COMPLETE ARCHITECTURE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nThe SupportMax Pro memory system demonstrates:\")\n",
    "print(\"  â€¢ Three-tier memory architecture (Working/Operational/Strategic)\")\n",
    "print(\"  â€¢ Episodic and semantic memory integration\")\n",
    "print(\"  â€¢ Vector-based semantic search\")\n",
    "print(\"  â€¢ Distributed state management\")\n",
    "print(\"  â€¢ Automatic memory consolidation\")\n",
    "print(\"  â€¢ Comprehensive performance metrics\")\n",
    "print(\"\\nThis architecture enables:\")\n",
    "print(\"  âœ“ Sub-100ms memory access\")\n",
    "print(\"  âœ“ Context-aware responses\")\n",
    "print(\"  âœ“ Continuous learning from resolutions\")\n",
    "print(\"  âœ“ Scalable concurrent agent operations\")\n",
    "print(\"  âœ“ Production-grade reliability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "This notebook demonstrated the key concepts from Chapter 4:\n",
    "\n",
    "1. **Memory Hierarchy**: Working, Session, and Long-term Memory\n",
    "2. **Cognitive Patterns**: Episodic and Semantic Memory\n",
    "3. **Memory Consolidation**: Transforming experiences into knowledge\n",
    "4. **Vector Databases**: Semantic search capabilities\n",
    "5. **Modern Frameworks**: Mem0-style intelligent memory\n",
    "6. **Distributed Systems**: State management across agents\n",
    "7. **Performance Metrics**: Monitoring and optimization\n",
    "8. **Complete Architecture**: Production-ready memory system\n",
    "\n",
    "### Key Takeaways for SupportMax Pro:\n",
    "\n",
    "- **80%+ cache hit rate** enables sub-10ms responses\n",
    "- **Semantic search** finds similar issues across 18M+ tickets\n",
    "- **Nightly consolidation** creates actionable knowledge from daily tickets\n",
    "- **Distributed locks** ensure consistency across 150+ concurrent agents\n",
    "- **Three-tier architecture** balances performance, cost, and comprehensiveness\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Chapter 5**: Distributed Context Management at scale\n",
    "2. **Chapter 6**: Advanced memory patterns with LangGraph\n",
    "3. **Chapter 12**: Production monitoring and observability\n",
    "\n",
    "### Try It Yourself:\n",
    "\n",
    "Modify the code to:\n",
    "- Add more sophisticated consolidation logic\n",
    "- Implement temporal decay for memory importance\n",
    "- Add multi-modal context (images, voice)\n",
    "- Optimize vector search with quantization\n",
    "- Implement GDPR compliance patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
