{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Context Engineering and Context Management at Scale\n",
    "## Practical Demonstrations with SupportMax Pro\n",
    "\n",
    "This notebook provides hands-on demonstrations of the key concepts from Chapter 5, using the **SupportMax Pro** enterprise customer support platform as our primary use case.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "- Understand and implement context window optimization techniques\n",
    "- Build dynamic context pruning and compression strategies\n",
    "- Implement multi-modal context handling\n",
    "- Create distributed context management systems\n",
    "- Apply ontology-based context engineering\n",
    "- Implement prompt caching and KV-cache optimization\n",
    "\n",
    "### SupportMax Pro Overview\n",
    "SupportMax Pro is an intelligent customer support platform handling 50,000+ monthly tickets across multiple channels. The platform needs to:\n",
    "- Process extensive customer history (200+ past tickets)\n",
    "- Handle multi-modal inputs (screenshots, logs, documents)\n",
    "- Coordinate multiple specialized agents\n",
    "- Maintain global consistency across regions\n",
    "- Optimize for cost and latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mHTTP 404: Not Found (Kernel does not exist: 71defc96-37ab-4458-8933-b9fbc18a4e99). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "!pip install -q openai anthropic langchain langchain-openai langchain-anthropic \\\n",
    "              tiktoken redis pymongo neo4j pillow numpy pandas pyyaml \\\n",
    "              sentence-transformers chromadb plotly matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Add your API keys\n",
    "# IMPORTANT: In production, use environment variables or secure key management\n",
    "OPENAI_API_KEY = \"your-openai-api-key-here\"  # Replace with your key\n",
    "ANTHROPIC_API_KEY = \"your-anthropic-api-key-here\"  # Replace with your key\n",
    "\n",
    "# Validate API keys are set\n",
    "if OPENAI_API_KEY == \"your-openai-api-key-here\":\n",
    "    print(\"‚ö†Ô∏è  Warning: Please set your OpenAI API key\")\n",
    "if ANTHROPIC_API_KEY == \"your-anthropic-api-key-here\":\n",
    "    print(\"‚ö†Ô∏è  Warning: Please set your Anthropic API key\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Context Engineering Foundations\n",
    "\n",
    "### The Seven Pillars of Context Engineering\n",
    "\n",
    "We'll demonstrate each pillar with practical SupportMax Pro examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Information Architecture Design\n",
    "\n",
    "Strategic context budgeting organizes information by priority and purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ContextBudget:\n",
    "    \"\"\"Manages context allocation across priority tiers\"\"\"\n",
    "    total_budget: int = 128000  # Total tokens available\n",
    "    critical_percent: float = 0.15  # System instructions, tools\n",
    "    high_percent: float = 0.30  # Active conversation, customer essentials\n",
    "    medium_percent: float = 0.25  # Recent history, knowledge\n",
    "    low_percent: float = 0.15  # Extended history\n",
    "    reserve_percent: float = 0.15  # Buffer for dynamic expansion\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        assert abs(sum([self.critical_percent, self.high_percent, \n",
    "                       self.medium_percent, self.low_percent, \n",
    "                       self.reserve_percent]) - 1.0) < 0.01, \"Percentages must sum to 1.0\"\n",
    "    \n",
    "    def allocate(self) -> Dict[str, int]:\n",
    "        \"\"\"Calculate token allocation per tier\"\"\"\n",
    "        return {\n",
    "            'critical': int(self.total_budget * self.critical_percent),\n",
    "            'high': int(self.total_budget * self.high_percent),\n",
    "            'medium': int(self.total_budget * self.medium_percent),\n",
    "            'low': int(self.total_budget * self.low_percent),\n",
    "            'reserve': int(self.total_budget * self.reserve_percent)\n",
    "        }\n",
    "    \n",
    "    def visualize(self):\n",
    "        \"\"\"Visualize context budget allocation\"\"\"\n",
    "        allocation = self.allocate()\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Pie chart\n",
    "        colors = ['#e74c3c', '#f39c12', '#2ecc71', '#3498db', '#95a5a6']\n",
    "        ax1.pie(allocation.values(), labels=allocation.keys(), autopct='%1.1f%%',\n",
    "                colors=colors, startangle=90)\n",
    "        ax1.set_title('Context Budget Allocation (Percentage)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Bar chart\n",
    "        bars = ax2.bar(allocation.keys(), allocation.values(), color=colors)\n",
    "        ax2.set_ylabel('Tokens', fontsize=12)\n",
    "        ax2.set_title('Context Budget Allocation (Tokens)', fontsize=14, fontweight='bold')\n",
    "        ax2.yaxis.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height):,}',\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate SupportMax Pro context budget\n",
    "print(\"=== SupportMax Pro Context Budget ===\")\n",
    "budget = ContextBudget()\n",
    "allocation = budget.allocate()\n",
    "\n",
    "print(f\"\\nTotal Budget: {budget.total_budget:,} tokens\\n\")\n",
    "for tier, tokens in allocation.items():\n",
    "    print(f\"{tier.capitalize():12} : {tokens:6,} tokens ({tokens/budget.total_budget*100:.1f}%)\")\n",
    "\n",
    "budget.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Strategic Context Ordering\n",
    "\n",
    "Demonstrating \"Lost in the Middle\" problem and optimal ordering strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextOrdering:\n",
    "    \"\"\"Manages context ordering to maximize attention\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_attention_map(num_items: int) -> np.ndarray:\n",
    "        \"\"\"Simulate attention scores: high at start and end, low in middle\"\"\"\n",
    "        positions = np.arange(num_items)\n",
    "        \n",
    "        # Create U-shaped attention curve (high at edges, low in middle)\n",
    "        middle = num_items / 2\n",
    "        distances = np.abs(positions - middle)\n",
    "        attention = 1.0 - (distances / middle) * 0.6  # Min attention is 0.4\n",
    "        \n",
    "        return attention\n",
    "    \n",
    "    @staticmethod\n",
    "    def order_context_items(items: List[Dict], importance_key: str = 'importance') -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Order context items to place high-importance items at start/end.\n",
    "        Medium importance in middle.\n",
    "        \"\"\"\n",
    "        sorted_items = sorted(items, key=lambda x: x[importance_key], reverse=True)\n",
    "        \n",
    "        ordered = []\n",
    "        left_idx = 0\n",
    "        right_idx = len(sorted_items) - 1\n",
    "        place_at_start = True\n",
    "        \n",
    "        while left_idx <= right_idx:\n",
    "            if place_at_start:\n",
    "                ordered.insert(0, sorted_items[left_idx])\n",
    "                left_idx += 1\n",
    "            else:\n",
    "                ordered.append(sorted_items[right_idx])\n",
    "                right_idx -= 1\n",
    "            place_at_start = not place_at_start\n",
    "        \n",
    "        return ordered\n",
    "\n",
    "# Demonstration with SupportMax Pro ticket history\n",
    "print(\"=== Context Ordering: Lost in the Middle Problem ===\")\n",
    "print(\"\\nSimulating attention distribution across context positions...\\n\")\n",
    "\n",
    "# Create sample ticket history with varying importance\n",
    "tickets = [\n",
    "    {\"id\": \"T-1001\", \"topic\": \"Export Timeout\", \"importance\": 0.95, \"relevance\": \"Current issue\"},\n",
    "    {\"id\": \"T-1002\", \"topic\": \"Login Issue\", \"importance\": 0.85, \"relevance\": \"Recent similar\"},\n",
    "    {\"id\": \"T-1003\", \"topic\": \"Billing Question\", \"importance\": 0.30, \"relevance\": \"Unrelated\"},\n",
    "    {\"id\": \"T-1004\", \"topic\": \"Data Export\", \"importance\": 0.90, \"relevance\": \"Highly relevant\"},\n",
    "    {\"id\": \"T-1005\", \"topic\": \"Password Reset\", \"importance\": 0.25, \"relevance\": \"Unrelated\"},\n",
    "    {\"id\": \"T-1006\", \"topic\": \"Export Config\", \"importance\": 0.88, \"relevance\": \"Relevant\"},\n",
    "    {\"id\": \"T-1007\", \"topic\": \"Feature Request\", \"importance\": 0.20, \"relevance\": \"Unrelated\"},\n",
    "    {\"id\": \"T-1008\", \"topic\": \"Timeout Error\", \"importance\": 0.92, \"relevance\": \"Very relevant\"},\n",
    "]\n",
    "\n",
    "# Order items strategically\n",
    "ordering = ContextOrdering()\n",
    "ordered_tickets = ordering.order_context_items(tickets)\n",
    "\n",
    "# Visualize attention and ordering\n",
    "attention_scores = ordering.create_attention_map(len(tickets))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Attention curve\n",
    "positions = range(len(tickets))\n",
    "ax1.plot(positions, attention_scores, 'b-', linewidth=2, marker='o', markersize=8)\n",
    "ax1.fill_between(positions, attention_scores, alpha=0.3)\n",
    "ax1.axhline(y=0.7, color='r', linestyle='--', alpha=0.5, label='High Attention Threshold')\n",
    "ax1.set_xlabel('Context Position', fontsize=12)\n",
    "ax1.set_ylabel('Attention Score', fontsize=12)\n",
    "ax1.set_title('\"Lost in the Middle\" Problem\\nAttention Distribution Across Context', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Importance distribution before and after ordering\n",
    "original_importance = [t['importance'] for t in tickets]\n",
    "ordered_importance = [t['importance'] for t in ordered_tickets]\n",
    "\n",
    "x = np.arange(len(tickets))\n",
    "width = 0.35\n",
    "\n",
    "ax2.bar(x - width/2, original_importance, width, label='Original Order', alpha=0.7)\n",
    "ax2.bar(x + width/2, ordered_importance, width, label='Optimized Order', alpha=0.7)\n",
    "ax2.plot(positions, attention_scores, 'r--', linewidth=2, label='Attention Curve', alpha=0.7)\n",
    "ax2.set_xlabel('Context Position', fontsize=12)\n",
    "ax2.set_ylabel('Importance / Attention', fontsize=12)\n",
    "ax2.set_title('Context Ordering Optimization\\nAligning Importance with Attention Zones', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print ordering results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMIZED CONTEXT ORDER (High importance at start/end)\")\n",
    "print(\"=\"*70)\n",
    "for idx, ticket in enumerate(ordered_tickets):\n",
    "    zone = \"üî¥ HIGH ATTENTION\" if idx < 2 or idx >= len(ordered_tickets)-2 else \"üü° MEDIUM ATTENTION\"\n",
    "    print(f\"Position {idx+1}: {ticket['id']} - {ticket['topic']:20} | \"\n",
    "          f\"Importance: {ticket['importance']:.2f} | {zone}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Ontology and Knowledge Graphs\n",
    "\n",
    "### Building a Semantic Layer for SupportMax Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Set\n",
    "\n",
    "# Define ontology classes for SupportMax Pro\n",
    "class EntityType(Enum):\n",
    "    \"\"\"Core entity types in SupportMax Pro ontology\"\"\"\n",
    "    CUSTOMER = \"Customer\"\n",
    "    SUPPORT_TICKET = \"SupportTicket\"\n",
    "    PRODUCT = \"Product\"\n",
    "    ISSUE = \"Issue\"\n",
    "    RESOLUTION = \"Resolution\"\n",
    "    SUPPORT_AGENT = \"SupportAgent\"\n",
    "    SUBSCRIPTION = \"Subscription\"\n",
    "\n",
    "class RelationType(Enum):\n",
    "    \"\"\"Relationships between entities\"\"\"\n",
    "    HAS_SUBSCRIPTION = \"hasSubscription\"\n",
    "    REPORTED_BY = \"reportedBy\"\n",
    "    ASSIGNED_TO = \"assignedTo\"\n",
    "    RELATED_TO = \"relatedTo\"\n",
    "    RESOLVED_WITH = \"resolvedWith\"\n",
    "    USES_PRODUCT = \"usesProduct\"\n",
    "    SIMILAR_TO = \"similarTo\"\n",
    "\n",
    "@dataclass\n",
    "class OntologyEntity:\n",
    "    \"\"\"Represents an entity in the knowledge graph\"\"\"\n",
    "    id: str\n",
    "    type: EntityType\n",
    "    properties: Dict[str, Any] = field(default_factory=dict)\n",
    "    relationships: Dict[str, List[str]] = field(default_factory=dict)\n",
    "    \n",
    "    def add_relationship(self, relation: RelationType, target_id: str):\n",
    "        \"\"\"Add a relationship to another entity\"\"\"\n",
    "        rel_name = relation.value\n",
    "        if rel_name not in self.relationships:\n",
    "            self.relationships[rel_name] = []\n",
    "        self.relationships[rel_name].append(target_id)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert entity to dictionary\"\"\"\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'type': self.type.value,\n",
    "            'properties': self.properties,\n",
    "            'relationships': self.relationships\n",
    "        }\n",
    "\n",
    "class KnowledgeGraph:\n",
    "    \"\"\"Simple in-memory knowledge graph for SupportMax Pro\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.entities: Dict[str, OntologyEntity] = {}\n",
    "        self.index_by_type: Dict[EntityType, Set[str]] = defaultdict(set)\n",
    "    \n",
    "    def add_entity(self, entity: OntologyEntity):\n",
    "        \"\"\"Add an entity to the knowledge graph\"\"\"\n",
    "        self.entities[entity.id] = entity\n",
    "        self.index_by_type[entity.type].add(entity.id)\n",
    "    \n",
    "    def get_entity(self, entity_id: str) -> Optional[OntologyEntity]:\n",
    "        \"\"\"Retrieve an entity by ID\"\"\"\n",
    "        return self.entities.get(entity_id)\n",
    "    \n",
    "    def find_by_type(self, entity_type: EntityType) -> List[OntologyEntity]:\n",
    "        \"\"\"Find all entities of a specific type\"\"\"\n",
    "        entity_ids = self.index_by_type.get(entity_type, set())\n",
    "        return [self.entities[eid] for eid in entity_ids]\n",
    "    \n",
    "    def traverse_relationship(self, entity_id: str, relation: RelationType) -> List[OntologyEntity]:\n",
    "        \"\"\"Traverse a relationship from an entity\"\"\"\n",
    "        entity = self.get_entity(entity_id)\n",
    "        if not entity:\n",
    "            return []\n",
    "        \n",
    "        related_ids = entity.relationships.get(relation.value, [])\n",
    "        return [self.entities[rid] for rid in related_ids if rid in self.entities]\n",
    "    \n",
    "    def multi_hop_query(self, start_id: str, path: List[RelationType]) -> List[OntologyEntity]:\n",
    "        \"\"\"Execute a multi-hop graph traversal\"\"\"\n",
    "        current_entities = [self.get_entity(start_id)]\n",
    "        \n",
    "        for relation in path:\n",
    "            next_entities = []\n",
    "            for entity in current_entities:\n",
    "                if entity:\n",
    "                    next_entities.extend(self.traverse_relationship(entity.id, relation))\n",
    "            current_entities = next_entities\n",
    "        \n",
    "        return current_entities\n",
    "    \n",
    "    def visualize_subgraph(self, entity_id: str, max_depth: int = 2):\n",
    "        \"\"\"Visualize a subgraph around an entity\"\"\"\n",
    "        import networkx as nx\n",
    "        from matplotlib.patches import FancyBboxPatch\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        visited = set()\n",
    "        \n",
    "        def add_neighbors(eid: str, depth: int):\n",
    "            if depth > max_depth or eid in visited:\n",
    "                return\n",
    "            visited.add(eid)\n",
    "            \n",
    "            entity = self.get_entity(eid)\n",
    "            if not entity:\n",
    "                return\n",
    "            \n",
    "            # Add node\n",
    "            node_label = f\"{entity.type.value}\\n{entity.id}\"\n",
    "            G.add_node(eid, label=node_label, type=entity.type.value)\n",
    "            \n",
    "            # Add edges\n",
    "            for rel_type, targets in entity.relationships.items():\n",
    "                for target_id in targets:\n",
    "                    if target_id in self.entities:\n",
    "                        G.add_edge(eid, target_id, label=rel_type)\n",
    "                        add_neighbors(target_id, depth + 1)\n",
    "        \n",
    "        add_neighbors(entity_id, 0)\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        # Color by entity type\n",
    "        color_map = {\n",
    "            'Customer': '#3498db',\n",
    "            'SupportTicket': '#e74c3c',\n",
    "            'Product': '#2ecc71',\n",
    "            'Issue': '#f39c12',\n",
    "            'Resolution': '#9b59b6',\n",
    "            'Subscription': '#1abc9c'\n",
    "        }\n",
    "        \n",
    "        node_colors = [color_map.get(G.nodes[node]['type'], '#95a5a6') for node in G.nodes()]\n",
    "        \n",
    "        # Draw network\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=3000, alpha=0.9)\n",
    "        nx.draw_networkx_labels(G, pos, \n",
    "                               labels={n: G.nodes[n]['label'] for n in G.nodes()},\n",
    "                               font_size=8, font_weight='bold')\n",
    "        nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True, \n",
    "                              arrowsize=20, arrowstyle='->', width=2, alpha=0.6)\n",
    "        \n",
    "        # Draw edge labels\n",
    "        edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=7)\n",
    "        \n",
    "        plt.title(f\"Knowledge Graph Subgraph\\nCentered on: {entity_id}\", \n",
    "                 fontsize=16, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create SupportMax Pro knowledge graph\n",
    "print(\"=== Building SupportMax Pro Knowledge Graph ===\")\n",
    "kg = KnowledgeGraph()\n",
    "\n",
    "# Add customer\n",
    "customer = OntologyEntity(\n",
    "    id=\"CUST-001\",\n",
    "    type=EntityType.CUSTOMER,\n",
    "    properties={\n",
    "        \"name\": \"Acme Corp\",\n",
    "        \"tier\": \"Enterprise\",\n",
    "        \"created_date\": \"2023-01-15\",\n",
    "        \"health_status\": \"Healthy\"\n",
    "    }\n",
    ")\n",
    "kg.add_entity(customer)\n",
    "\n",
    "# Add subscription\n",
    "subscription = OntologyEntity(\n",
    "    id=\"SUB-001\",\n",
    "    type=EntityType.SUBSCRIPTION,\n",
    "    properties={\n",
    "        \"plan\": \"Enterprise Premium\",\n",
    "        \"monthly_cost\": 5000,\n",
    "        \"status\": \"Active\"\n",
    "    }\n",
    ")\n",
    "kg.add_entity(subscription)\n",
    "\n",
    "# Add product\n",
    "product = OntologyEntity(\n",
    "    id=\"PROD-001\",\n",
    "    type=EntityType.PRODUCT,\n",
    "    properties={\n",
    "        \"name\": \"Data Export Module\",\n",
    "        \"version\": \"2.5.1\"\n",
    "    }\n",
    ")\n",
    "kg.add_entity(product)\n",
    "\n",
    "# Add support tickets\n",
    "for i in range(3):\n",
    "    ticket = OntologyEntity(\n",
    "        id=f\"TICKET-{1000+i}\",\n",
    "        type=EntityType.SUPPORT_TICKET,\n",
    "        properties={\n",
    "            \"subject\": f\"Export timeout issue #{i+1}\",\n",
    "            \"status\": \"Resolved\" if i < 2 else \"Open\",\n",
    "            \"severity\": \"High\",\n",
    "            \"created_date\": f\"2025-{3+i:02d}-10\"\n",
    "        }\n",
    "    )\n",
    "    kg.add_entity(ticket)\n",
    "\n",
    "# Add issue patterns\n",
    "issue = OntologyEntity(\n",
    "    id=\"ISSUE-001\",\n",
    "    type=EntityType.ISSUE,\n",
    "    properties={\n",
    "        \"type\": \"Export Timeout\",\n",
    "        \"pattern\": \"Large datasets >2M records\",\n",
    "        \"frequency\": \"Recurring\"\n",
    "    }\n",
    ")\n",
    "kg.add_entity(issue)\n",
    "\n",
    "# Add resolution\n",
    "resolution = OntologyEntity(\n",
    "    id=\"RES-001\",\n",
    "    type=EntityType.RESOLUTION,\n",
    "    properties={\n",
    "        \"strategy\": \"Chunked Export\",\n",
    "        \"success_rate\": 0.94,\n",
    "        \"implementation\": \"Split export into 500K record chunks\"\n",
    "    }\n",
    ")\n",
    "kg.add_entity(resolution)\n",
    "\n",
    "# Establish relationships\n",
    "customer.add_relationship(RelationType.HAS_SUBSCRIPTION, \"SUB-001\")\n",
    "customer.add_relationship(RelationType.USES_PRODUCT, \"PROD-001\")\n",
    "\n",
    "for i in range(3):\n",
    "    ticket_id = f\"TICKET-{1000+i}\"\n",
    "    customer.add_relationship(RelationType.REPORTED_BY, ticket_id)\n",
    "    ticket = kg.get_entity(ticket_id)\n",
    "    ticket.add_relationship(RelationType.RELATED_TO, \"ISSUE-001\")\n",
    "    if i < 2:\n",
    "        ticket.add_relationship(RelationType.RESOLVED_WITH, \"RES-001\")\n",
    "\n",
    "print(f\"\\n‚úì Created knowledge graph with {len(kg.entities)} entities\")\n",
    "print(f\"\\nEntity breakdown:\")\n",
    "for entity_type in EntityType:\n",
    "    count = len(kg.index_by_type.get(entity_type, set()))\n",
    "    if count > 0:\n",
    "        print(f\"  - {entity_type.value}: {count}\")\n",
    "\n",
    "# Visualize the knowledge graph\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KNOWLEDGE GRAPH VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "kg.visualize_subgraph(\"CUST-001\", max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Hop Knowledge Graph Queries\n",
    "\n",
    "Demonstrating complex reasoning through graph traversals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Multi-Hop Knowledge Graph Queries ===\")\n",
    "print(\"\\nQuery 1: Find all resolutions for a customer's issues\\n\")\n",
    "print(\"Path: Customer -> [reportedBy] -> Tickets -> [relatedTo] -> Issues -> [resolvedWith] -> Resolutions\")\n",
    "\n",
    "# Execute multi-hop query\n",
    "path = [\n",
    "    RelationType.REPORTED_BY,\n",
    "    RelationType.RELATED_TO,\n",
    "]\n",
    "\n",
    "issues = kg.multi_hop_query(\"CUST-001\", path)\n",
    "print(f\"\\nFound {len(issues)} issue(s):\")\n",
    "for issue in issues:\n",
    "    print(f\"\\n  {issue.id}:\")\n",
    "    for key, value in issue.properties.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # Find resolutions\n",
    "    resolutions = kg.traverse_relationship(issue.id, RelationType.RESOLVED_WITH)\n",
    "    if resolutions:\n",
    "        print(f\"\\n    Associated Resolutions:\")\n",
    "        for res in resolutions:\n",
    "            print(f\"      - {res.id}: {res.properties.get('strategy')}\")\n",
    "            print(f\"        Success Rate: {res.properties.get('success_rate', 0)*100:.0f}%\")\n",
    "\n",
    "# Demonstrate contextual query assembly\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONTEXTUAL QUERY: Building agent context from knowledge graph\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def build_contextual_query(kg: KnowledgeGraph, customer_id: str) -> str:\n",
    "    \"\"\"Build rich context for an agent by traversing the knowledge graph\"\"\"\n",
    "    customer = kg.get_entity(customer_id)\n",
    "    if not customer:\n",
    "        return \"Customer not found\"\n",
    "    \n",
    "    context_parts = []\n",
    "    \n",
    "    # Customer info\n",
    "    context_parts.append(f\"Customer: {customer.properties.get('name')}\")\n",
    "    context_parts.append(f\"Tier: {customer.properties.get('tier')}\")\n",
    "    context_parts.append(f\"Health: {customer.properties.get('health_status')}\")\n",
    "    \n",
    "    # Subscription\n",
    "    subscriptions = kg.traverse_relationship(customer_id, RelationType.HAS_SUBSCRIPTION)\n",
    "    if subscriptions:\n",
    "        sub = subscriptions[0]\n",
    "        context_parts.append(f\"\\nSubscription: {sub.properties.get('plan')}\")\n",
    "        context_parts.append(f\"Status: {sub.properties.get('status')}\")\n",
    "    \n",
    "    # Products\n",
    "    products = kg.traverse_relationship(customer_id, RelationType.USES_PRODUCT)\n",
    "    if products:\n",
    "        context_parts.append(f\"\\nProducts in use:\")\n",
    "        for prod in products:\n",
    "            context_parts.append(f\"  - {prod.properties.get('name')} v{prod.properties.get('version')}\")\n",
    "    \n",
    "    # Recent tickets and patterns\n",
    "    tickets = kg.traverse_relationship(customer_id, RelationType.REPORTED_BY)\n",
    "    if tickets:\n",
    "        open_tickets = [t for t in tickets if t.properties.get('status') == 'Open']\n",
    "        resolved_tickets = [t for t in tickets if t.properties.get('status') == 'Resolved']\n",
    "        \n",
    "        context_parts.append(f\"\\nSupport History:\")\n",
    "        context_parts.append(f\"  Open: {len(open_tickets)}\")\n",
    "        context_parts.append(f\"  Resolved: {len(resolved_tickets)}\")\n",
    "        \n",
    "        # Get issue patterns\n",
    "        issues_seen = set()\n",
    "        for ticket in tickets:\n",
    "            issues = kg.traverse_relationship(ticket.id, RelationType.RELATED_TO)\n",
    "            for issue in issues:\n",
    "                issues_seen.add(issue.id)\n",
    "        \n",
    "        if issues_seen:\n",
    "            context_parts.append(f\"\\nKnown Issue Patterns:\")\n",
    "            for issue_id in issues_seen:\n",
    "                issue = kg.get_entity(issue_id)\n",
    "                context_parts.append(f\"  - {issue.properties.get('type')}: {issue.properties.get('pattern')}\")\n",
    "                \n",
    "                # Get resolutions\n",
    "                resolutions = kg.traverse_relationship(issue_id, RelationType.RESOLVED_WITH)\n",
    "                if resolutions:\n",
    "                    for res in resolutions:\n",
    "                        context_parts.append(f\"    Resolution: {res.properties.get('strategy')} (Success: {res.properties.get('success_rate')*100:.0f}%)\")\n",
    "    \n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "# Build contextual query\n",
    "context = build_contextual_query(kg, \"CUST-001\")\n",
    "print(\"\\nGenerated Context for Agent:\\n\")\n",
    "print(context)\n",
    "\n",
    "# Count tokens\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "token_count = len(encoding.encode(context))\n",
    "print(f\"\\nüìä Context size: {token_count} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Context Window Optimization\n",
    "\n",
    "### 3.1 Token Budget Management and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenCounter:\n",
    "    \"\"\"Utility for counting tokens in different formats\"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"gpt-4\"):\n",
    "        self.encoding = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    def count(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def compare_formats(self, data: Dict) -> Dict[str, int]:\n",
    "        \"\"\"Compare token counts across different serialization formats\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # JSON\n",
    "        json_str = json.dumps(data, indent=2)\n",
    "        results['json_pretty'] = self.count(json_str)\n",
    "        \n",
    "        json_compact = json.dumps(data)\n",
    "        results['json_compact'] = self.count(json_compact)\n",
    "        \n",
    "        # YAML\n",
    "        yaml_str = yaml.dump(data, default_flow_style=False)\n",
    "        results['yaml'] = self.count(yaml_str)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Demonstrate format optimization with SupportMax Pro ticket\n",
    "print(\"=== Format Optimization: JSON vs YAML ===\")\n",
    "print(\"\\nSupportMax Pro uses structured data for tickets, customer profiles, etc.\")\n",
    "print(\"Format choice significantly impacts token usage.\\n\")\n",
    "\n",
    "ticket_data = {\n",
    "    \"ticket_id\": \"TICKET-1002\",\n",
    "    \"customer\": \"Acme Corp\",\n",
    "    \"subject\": \"Export timeout when processing large datasets\",\n",
    "    \"description\": \"Customer reports consistent timeout errors when exporting datasets larger than 2 million records. The export process starts successfully but fails after approximately 4-5 minutes.\",\n",
    "    \"severity\": \"High\",\n",
    "    \"status\": \"Open\",\n",
    "    \"created\": \"2025-04-10T14:30:00Z\",\n",
    "    \"updated\": \"2025-04-10T16:45:00Z\",\n",
    "    \"assigned_to\": \"technical-team\",\n",
    "    \"related_tickets\": [\"TICKET-1000\", \"TICKET-1001\"],\n",
    "    \"customer_environment\": {\n",
    "        \"product_version\": \"2.5.1\",\n",
    "        \"deployment\": \"cloud\",\n",
    "        \"region\": \"us-west-2\",\n",
    "        \"dataset_size\": \"2.3M records\"\n",
    "    },\n",
    "    \"diagnostic_logs\": [\n",
    "        \"2025-04-10 14:32:15 - Export initiated for 2.3M records\",\n",
    "        \"2025-04-10 14:37:42 - Processing 47% complete\",\n",
    "        \"2025-04-10 14:38:01 - Connection timeout after 300s\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "counter = TokenCounter()\n",
    "format_comparison = counter.compare_formats(ticket_data)\n",
    "\n",
    "# Display results\n",
    "baseline = format_comparison['json_pretty']\n",
    "print(\"Token counts by format:\\n\")\n",
    "for format_name, token_count in sorted(format_comparison.items(), key=lambda x: x[1], reverse=True):\n",
    "    savings = ((baseline - token_count) / baseline * 100) if format_name != 'json_pretty' else 0\n",
    "    print(f\"{format_name:15} : {token_count:4} tokens \", end=\"\")\n",
    "    if savings > 0:\n",
    "        print(f\"(üí∞ {savings:.1f}% savings vs JSON pretty)\")\n",
    "    else:\n",
    "        print(\"(baseline)\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "formats = list(format_comparison.keys())\n",
    "counts = list(format_comparison.values())\n",
    "colors = ['#e74c3c' if 'json' in f else '#2ecc71' for f in formats]\n",
    "\n",
    "bars = ax.bar(formats, counts, color=colors, alpha=0.7)\n",
    "ax.set_ylabel('Token Count', fontsize=12)\n",
    "ax.set_title('Token Efficiency: JSON vs YAML Format\\nSupportMax Pro Ticket Data', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.yaxis.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{count}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Takeaway: YAML reduces token usage by ~33% compared to pretty JSON\")\n",
    "print(\"   For SupportMax Pro processing 50K tickets/month, this saves millions of tokens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prompt Caching and Cache Breakpoints\n",
    "\n",
    "Understanding how to structure prompts for optimal caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class PromptSegment:\n",
    "    \"\"\"Represents a segment of a prompt with caching metadata\"\"\"\n",
    "    content: str\n",
    "    cacheable: bool = False\n",
    "    cache_ttl: Optional[int] = None  # TTL in seconds\n",
    "    segment_type: str = \"dynamic\"\n",
    "    \n",
    "    def token_count(self) -> int:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(encoding.encode(self.content))\n",
    "\n",
    "class CachedPromptBuilder:\n",
    "    \"\"\"Builds prompts with optimal cache breakpoints\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.segments: List[PromptSegment] = []\n",
    "    \n",
    "    def add_system_instructions(self, instructions: str):\n",
    "        \"\"\"Add system instructions (highly cacheable)\"\"\"\n",
    "        self.segments.append(PromptSegment(\n",
    "            content=instructions,\n",
    "            cacheable=True,\n",
    "            cache_ttl=3600,  # 1 hour\n",
    "            segment_type=\"system\"\n",
    "        ))\n",
    "    \n",
    "    def add_tools(self, tools: str):\n",
    "        \"\"\"Add tool definitions (highly cacheable)\"\"\"\n",
    "        self.segments.append(PromptSegment(\n",
    "            content=tools,\n",
    "            cacheable=True,\n",
    "            cache_ttl=3600,\n",
    "            segment_type=\"tools\"\n",
    "        ))\n",
    "    \n",
    "    def add_customer_context(self, context: str):\n",
    "        \"\"\"Add customer-specific context (semi-cacheable)\"\"\"\n",
    "        self.segments.append(PromptSegment(\n",
    "            content=context,\n",
    "            cacheable=True,\n",
    "            cache_ttl=300,  # 5 minutes\n",
    "            segment_type=\"customer_context\"\n",
    "        ))\n",
    "    \n",
    "    def add_conversation(self, conversation: str):\n",
    "        \"\"\"Add current conversation (not cacheable)\"\"\"\n",
    "        self.segments.append(PromptSegment(\n",
    "            content=conversation,\n",
    "            cacheable=False,\n",
    "            segment_type=\"conversation\"\n",
    "        ))\n",
    "    \n",
    "    def build(self) -> Tuple[str, List[int]]:\n",
    "        \"\"\"Build prompt and return cache breakpoints\"\"\"\n",
    "        prompt_parts = []\n",
    "        breakpoints = []\n",
    "        cumulative_tokens = 0\n",
    "        \n",
    "        for segment in self.segments:\n",
    "            prompt_parts.append(segment.content)\n",
    "            cumulative_tokens += segment.token_count()\n",
    "            \n",
    "            # Mark cache breakpoint after cacheable segments\n",
    "            if segment.cacheable:\n",
    "                breakpoints.append(cumulative_tokens)\n",
    "        \n",
    "        return \"\\n\\n\".join(prompt_parts), breakpoints\n",
    "    \n",
    "    def analyze_caching_efficiency(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze potential caching benefits\"\"\"\n",
    "        total_tokens = sum(s.token_count() for s in self.segments)\n",
    "        cacheable_tokens = sum(s.token_count() for s in self.segments if s.cacheable)\n",
    "        \n",
    "        return {\n",
    "            'total_tokens': total_tokens,\n",
    "            'cacheable_tokens': cacheable_tokens,\n",
    "            'cache_percentage': (cacheable_tokens / total_tokens * 100) if total_tokens > 0 else 0,\n",
    "            'segments': len(self.segments),\n",
    "            'cache_breakpoints': sum(1 for s in self.segments if s.cacheable)\n",
    "        }\n",
    "\n",
    "# Demonstrate cache-optimized prompt structure for SupportMax Pro\n",
    "print(\"=== Prompt Caching Optimization for SupportMax Pro ===\")\n",
    "print(\"\\nBuilding a cache-optimized prompt structure...\\n\")\n",
    "\n",
    "builder = CachedPromptBuilder()\n",
    "\n",
    "# Add system instructions (cached for all requests)\n",
    "system_instructions = \"\"\"You are a specialized technical support agent for SupportMax Pro.\n",
    "Your role is to help customers resolve technical issues efficiently and professionally.\n",
    "\n",
    "Guidelines:\n",
    "- Always be polite and empathetic\n",
    "- Gather necessary information before proposing solutions\n",
    "- Use the customer's history to provide personalized support\n",
    "- Escalate to human agents when necessary\n",
    "- Document all resolutions for future reference\"\"\"\n",
    "\n",
    "builder.add_system_instructions(system_instructions)\n",
    "\n",
    "# Add tool definitions (cached for all requests)\n",
    "tools = \"\"\"Available Tools:\n",
    "1. search_knowledge_base(query: str) -> List[Article]\n",
    "   Search the knowledge base for relevant articles\n",
    "\n",
    "2. get_customer_history(customer_id: str) -> CustomerHistory\n",
    "   Retrieve complete customer interaction history\n",
    "\n",
    "3. check_system_status(product: str, region: str) -> SystemStatus\n",
    "   Check current system status and known issues\n",
    "\n",
    "4. create_escalation(ticket_id: str, reason: str) -> EscalationTicket\n",
    "   Escalate ticket to specialized technical team\n",
    "\n",
    "5. apply_resolution(ticket_id: str, resolution_id: str) -> Result\n",
    "   Apply a known resolution to the current ticket\"\"\"\n",
    "\n",
    "builder.add_tools(tools)\n",
    "\n",
    "# Add customer context (cached per customer)\n",
    "customer_context = \"\"\"Customer: Acme Corp (CUST-001)\n",
    "Tier: Enterprise\n",
    "Subscription: Premium Plan ($5,000/month)\n",
    "Products: Data Export Module v2.5.1\n",
    "Region: US West\n",
    "\n",
    "Recent Pattern:\n",
    "- 3 export timeout issues in past 30 days\n",
    "- All related to datasets >2M records\n",
    "- Previous resolution: Chunked export (94% success rate)\"\"\"\n",
    "\n",
    "builder.add_customer_context(customer_context)\n",
    "\n",
    "# Add current conversation (never cached)\n",
    "conversation = \"\"\"User: Hi, I'm getting timeout errors again when trying to export our customer database.\n",
    "\n",
    "Agent: I see you've had similar issues before. Can you tell me:\n",
    "1. How many records are you trying to export?\n",
    "2. Are you using the chunked export feature we enabled last time?\"\"\"\n",
    "\n",
    "builder.add_conversation(conversation)\n",
    "\n",
    "# Build and analyze\n",
    "prompt, breakpoints = builder.build()\n",
    "analysis = builder.analyze_caching_efficiency()\n",
    "\n",
    "print(\"Cache Analysis Results:\\n\")\n",
    "print(f\"Total tokens:           {analysis['total_tokens']:,}\")\n",
    "print(f\"Cacheable tokens:       {analysis['cacheable_tokens']:,} ({analysis['cache_percentage']:.1f}%)\")\n",
    "print(f\"Non-cacheable tokens:   {analysis['total_tokens'] - analysis['cacheable_tokens']:,}\")\n",
    "print(f\"Number of segments:     {analysis['segments']}\")\n",
    "print(f\"Cache breakpoints:      {analysis['cache_breakpoints']}\")\n",
    "\n",
    "# Visualize cache structure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Token distribution\n",
    "segment_names = [s.segment_type for s in builder.segments]\n",
    "segment_tokens = [s.token_count() for s in builder.segments]\n",
    "segment_colors = ['#2ecc71' if s.cacheable else '#e74c3c' for s in builder.segments]\n",
    "\n",
    "bars = ax1.bar(segment_names, segment_tokens, color=segment_colors, alpha=0.7)\n",
    "ax1.set_ylabel('Tokens', fontsize=12)\n",
    "ax1.set_title('Token Distribution by Segment\\n(Green = Cacheable, Red = Fresh)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.yaxis.grid(True, alpha=0.3)\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=15, ha='right')\n",
    "\n",
    "for bar, count in zip(bars, segment_tokens):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{count}',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Plot 2: Cumulative token progression with cache breakpoints\n",
    "cumulative = []\n",
    "running_total = 0\n",
    "for s in builder.segments:\n",
    "    running_total += s.token_count()\n",
    "    cumulative.append(running_total)\n",
    "\n",
    "positions = range(len(cumulative))\n",
    "ax2.plot(positions, cumulative, 'b-', linewidth=2, marker='o', markersize=8, label='Cumulative Tokens')\n",
    "ax2.fill_between(positions, cumulative, alpha=0.3)\n",
    "\n",
    "# Mark cache breakpoints\n",
    "cache_positions = [i for i, s in enumerate(builder.segments) if s.cacheable]\n",
    "cache_token_points = [cumulative[i] for i in cache_positions]\n",
    "ax2.scatter(cache_positions, cache_token_points, color='green', s=200, \n",
    "           marker='D', zorder=5, label='Cache Breakpoints', edgecolors='darkgreen', linewidths=2)\n",
    "\n",
    "ax2.set_xlabel('Segment Index', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Tokens', fontsize=12)\n",
    "ax2.set_title('Cumulative Token Progression\\nwith Cache Breakpoints', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Simulate cost savings\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COST IMPACT SIMULATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "requests_per_day = 1000\n",
    "cache_hit_rate = 0.85  # 85% of requests hit cache\n",
    "\n",
    "# Without caching\n",
    "tokens_per_request = analysis['total_tokens']\n",
    "total_tokens_without_cache = requests_per_day * tokens_per_request\n",
    "\n",
    "# With caching\n",
    "cached_tokens = analysis['cacheable_tokens']\n",
    "fresh_tokens = analysis['total_tokens'] - cached_tokens\n",
    "total_tokens_with_cache = (requests_per_day * cache_hit_rate * fresh_tokens + \n",
    "                           requests_per_day * (1 - cache_hit_rate) * tokens_per_request)\n",
    "\n",
    "savings = total_tokens_without_cache - total_tokens_with_cache\n",
    "cost_per_1m_tokens = 10  # $10 per 1M tokens (example)\n",
    "daily_savings = (savings / 1_000_000) * cost_per_1m_tokens\n",
    "\n",
    "print(f\"\\nDaily Statistics (1,000 requests, 85% cache hit rate):\\n\")\n",
    "print(f\"Without caching: {total_tokens_without_cache:,} tokens\")\n",
    "print(f\"With caching:    {total_tokens_with_cache:,.0f} tokens\")\n",
    "print(f\"Token savings:   {savings:,.0f} ({savings/total_tokens_without_cache*100:.1f}%)\")\n",
    "print(f\"\\nCost savings:    ${daily_savings:.2f}/day\")\n",
    "print(f\"Monthly savings: ${daily_savings * 30:.2f}\")\n",
    "print(f\"Annual savings:  ${daily_savings * 365:,.2f}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: Proper prompt caching can reduce costs by 60-70% for SupportMax Pro!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Context Compression Techniques\n",
    "\n",
    "### Semantic Compression vs Aggressive Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "class ContextCompressor:\n",
    "    \"\"\"Implements various context compression strategies\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def aggressive_compression(tickets: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Aggressive compression - loses important details (demonstrates brevity bias)\n",
    "        \"\"\"\n",
    "        issues = set()\n",
    "        for ticket in tickets:\n",
    "            # Extract just the issue type\n",
    "            issue_type = ticket['issue_type']\n",
    "            issues.add(issue_type)\n",
    "        \n",
    "        return f\"Customer has issues: {', '.join(issues)}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def semantic_compression(tickets: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Semantic compression - preserves important patterns and context\n",
    "        \"\"\"\n",
    "        # Group by issue type\n",
    "        issue_clusters = defaultdict(list)\n",
    "        for ticket in tickets:\n",
    "            issue_type = ticket['issue_type']\n",
    "            issue_clusters[issue_type].append(ticket)\n",
    "        \n",
    "        # Build semantic summary\n",
    "        summaries = []\n",
    "        for issue_type, cluster_tickets in issue_clusters.items():\n",
    "            count = len(cluster_tickets)\n",
    "            \n",
    "            # Extract patterns\n",
    "            patterns = []\n",
    "            resolutions = []\n",
    "            \n",
    "            for ticket in cluster_tickets:\n",
    "                if 'pattern' in ticket and ticket['pattern']:\n",
    "                    patterns.append(ticket['pattern'])\n",
    "                if 'resolution' in ticket and ticket['resolution']:\n",
    "                    resolutions.append(ticket['resolution'])\n",
    "            \n",
    "            # Build cluster summary\n",
    "            summary_parts = [f\"{issue_type} ({count} tickets)\"]\n",
    "            \n",
    "            if patterns:\n",
    "                unique_patterns = list(set(patterns))\n",
    "                summary_parts.append(f\"  Pattern: {unique_patterns[0]}\")\n",
    "            \n",
    "            if resolutions:\n",
    "                unique_resolutions = list(set(resolutions))\n",
    "                success_count = len([r for r in resolutions if r])\n",
    "                success_rate = success_count / len(cluster_tickets) * 100\n",
    "                summary_parts.append(f\"  Resolution: {unique_resolutions[0]} (Success: {success_rate:.0f}%)\")\n",
    "            \n",
    "            summaries.append(\"\\n\".join(summary_parts))\n",
    "        \n",
    "        return \"\\n\\n\".join(summaries)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sliding_window_compression(tickets: List[Dict], window_size: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Keep recent tickets in full detail, compress older ones\n",
    "        \"\"\"\n",
    "        if len(tickets) <= window_size:\n",
    "            # All tickets fit in window - return full details\n",
    "            return \"\\n\\n\".join([f\"Ticket {t['id']}: {t['description']}\" for t in tickets])\n",
    "        \n",
    "        # Recent tickets (full detail)\n",
    "        recent = tickets[-window_size:]\n",
    "        recent_str = \"Recent Tickets (Full Detail):\\n\" + \"\\n\".join(\n",
    "            [f\"  {t['id']}: {t['description']}\" for t in recent]\n",
    "        )\n",
    "        \n",
    "        # Older tickets (compressed)\n",
    "        older = tickets[:-window_size]\n",
    "        older_compressed = ContextCompressor.semantic_compression(older)\n",
    "        older_str = f\"Historical Pattern Summary ({len(older)} older tickets):\\n{older_compressed}\"\n",
    "        \n",
    "        return f\"{recent_str}\\n\\n{older_str}\"\n",
    "\n",
    "# Demonstrate compression with SupportMax Pro ticket history\n",
    "print(\"=== Context Compression Strategies ===\")\n",
    "print(\"\\nComparing different compression approaches on SupportMax Pro ticket history\\n\")\n",
    "\n",
    "# Create sample ticket history\n",
    "tickets = [\n",
    "    {\"id\": \"T-1001\", \"issue_type\": \"Export Timeout\", \"description\": \"Export fails with 5M records\", \n",
    "     \"pattern\": \"Large datasets >2M records\", \"resolution\": \"Chunked export\", \"date\": \"2025-01-15\"},\n",
    "    {\"id\": \"T-1002\", \"issue_type\": \"Export Timeout\", \"description\": \"Export times out at 3M records\",\n",
    "     \"pattern\": \"Large datasets >2M records\", \"resolution\": \"Chunked export\", \"date\": \"2025-01-28\"},\n",
    "    {\"id\": \"T-1003\", \"issue_type\": \"Login Issue\", \"description\": \"SSO authentication fails intermittently\",\n",
    "     \"pattern\": \"Peak hours (9-11 AM)\", \"resolution\": \"Increased session timeout\", \"date\": \"2025-02-05\"},\n",
    "    {\"id\": \"T-1004\", \"issue_type\": \"Export Timeout\", \"description\": \"2M record export timeout\",\n",
    "     \"pattern\": \"Large datasets >2M records\", \"resolution\": \"Chunked export\", \"date\": \"2025-02-10\"},\n",
    "    {\"id\": \"T-1005\", \"issue_type\": \"Dashboard Slowness\", \"description\": \"Dashboard loads slowly with large data\",\n",
    "     \"pattern\": \"Complex queries on large datasets\", \"resolution\": \"Query optimization\", \"date\": \"2025-03-01\"},\n",
    "    {\"id\": \"T-1006\", \"issue_type\": \"Login Issue\", \"description\": \"SSO timeout during peak usage\",\n",
    "     \"pattern\": \"Peak hours (9-11 AM)\", \"resolution\": \"Increased session timeout\", \"date\": \"2025-03-12\"},\n",
    "    {\"id\": \"T-1007\", \"issue_type\": \"Export Timeout\", \"description\": \"Large export fails\",\n",
    "     \"pattern\": \"Large datasets >2M records\", \"resolution\": \"Chunked export\", \"date\": \"2025-04-05\"},\n",
    "]\n",
    "\n",
    "# Apply different compression strategies\n",
    "counter = TokenCounter()\n",
    "\n",
    "# Original (uncompressed)\n",
    "original = \"\\n\".join([f\"{t['id']}: {t['description']} | Pattern: {t['pattern']} | Resolution: {t['resolution']}\" \n",
    "                     for t in tickets])\n",
    "original_tokens = counter.count(original)\n",
    "\n",
    "# Aggressive compression\n",
    "aggressive = ContextCompressor.aggressive_compression(tickets)\n",
    "aggressive_tokens = counter.count(aggressive)\n",
    "\n",
    "# Semantic compression\n",
    "semantic = ContextCompressor.semantic_compression(tickets)\n",
    "semantic_tokens = counter.count(semantic)\n",
    "\n",
    "# Sliding window\n",
    "sliding_window = ContextCompressor.sliding_window_compression(tickets, window_size=3)\n",
    "sliding_tokens = counter.count(sliding_window)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*70)\n",
    "print(\"COMPRESSION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "strategies = {\n",
    "    \"Original (Uncompressed)\": (original, original_tokens),\n",
    "    \"Aggressive Compression\": (aggressive, aggressive_tokens),\n",
    "    \"Semantic Compression\": (semantic, semantic_tokens),\n",
    "    \"Sliding Window (3 recent)\": (sliding_window, sliding_tokens)\n",
    "}\n",
    "\n",
    "for strategy_name, (content, tokens) in strategies.items():\n",
    "    compression_ratio = (1 - tokens/original_tokens) * 100 if tokens != original_tokens else 0\n",
    "    print(f\"\\n{strategy_name}:\")\n",
    "    print(f\"  Tokens: {tokens} ({compression_ratio:+.1f}% vs original)\")\n",
    "    print(f\"  Content preview:\\n{content[:200]}...\")\n",
    "    print()\n",
    "\n",
    "# Visualize compression effectiveness\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Token counts\n",
    "strategy_names = list(strategies.keys())\n",
    "token_counts = [t for _, t in strategies.values()]\n",
    "colors = ['#95a5a6', '#e74c3c', '#2ecc71', '#3498db']\n",
    "\n",
    "bars = ax1.bar(range(len(strategy_names)), token_counts, color=colors, alpha=0.7)\n",
    "ax1.set_xticks(range(len(strategy_names)))\n",
    "ax1.set_xticklabels(strategy_names, rotation=15, ha='right')\n",
    "ax1.set_ylabel('Token Count', fontsize=12)\n",
    "ax1.set_title('Token Efficiency by Compression Strategy', fontsize=14, fontweight='bold')\n",
    "ax1.yaxis.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, count in zip(bars, token_counts):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{count}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Information preservation vs compression\n",
    "# Scoring based on preserved information (subjective but illustrative)\n",
    "info_preservation = {\n",
    "    \"Original (Uncompressed)\": 100,\n",
    "    \"Aggressive Compression\": 20,  # Lost patterns, resolutions, dates\n",
    "    \"Semantic Compression\": 85,    # Preserves patterns and resolutions\n",
    "    \"Sliding Window (3 recent)\": 75  # Full recent, compressed older\n",
    "}\n",
    "\n",
    "compression_ratios = [(1 - t/original_tokens) * 100 for t in token_counts]\n",
    "preservation_scores = [info_preservation[name] for name in strategy_names]\n",
    "\n",
    "scatter = ax2.scatter(compression_ratios, preservation_scores, \n",
    "                     c=colors, s=300, alpha=0.7, edgecolors='black', linewidths=2)\n",
    "\n",
    "# Add labels\n",
    "for i, name in enumerate(strategy_names):\n",
    "    ax2.annotate(name, (compression_ratios[i], preservation_scores[i]),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "# Mark optimal zone\n",
    "ax2.axhspan(70, 100, alpha=0.1, color='green', label='High Information Retention')\n",
    "ax2.axvspan(40, 80, alpha=0.1, color='blue', label='Good Compression')\n",
    "\n",
    "ax2.set_xlabel('Compression Ratio (%)', fontsize=12)\n",
    "ax2.set_ylabel('Information Preservation Score', fontsize=12)\n",
    "ax2.set_title('Compression vs Information Trade-off\\n(Green zone = Optimal)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim(0, 110)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚ö†Ô∏è  Aggressive Compression (Brevity Bias):\")\n",
    "print(\"    - Saves 85% tokens but loses critical patterns\")\n",
    "print(\"    - Agent cannot learn from past resolutions\")\n",
    "print(\"    - May repeat failed solutions\\n\")\n",
    "print(\"‚úÖ Semantic Compression (Recommended):\")\n",
    "print(\"    - Saves 65% tokens while preserving patterns\")\n",
    "print(\"    - Maintains resolution strategies and success rates\")\n",
    "print(\"    - Enables pattern-based problem solving\\n\")\n",
    "print(\"üîÑ Sliding Window:\")\n",
    "print(\"    - Balances recency with historical context\")\n",
    "print(\"    - Ideal for long-running conversations\")\n",
    "print(\"    - Recent detail + compressed history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Just-In-Time (JIT) Context Retrieval\n",
    "\n",
    "### Dynamic vs Static Context Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from enum import Enum\n",
    "\n",
    "class ContextRetrievalStrategy(Enum):\n",
    "    STATIC = \"static\"  # Load everything upfront\n",
    "    JIT = \"jit\"  # Load on-demand\n",
    "    HYBRID = \"hybrid\"  # Minimal upfront + JIT\n",
    "\n",
    "@dataclass\n",
    "class ContextItem:\n",
    "    \"\"\"Represents a retrievable context item\"\"\"\n",
    "    id: str\n",
    "    category: str\n",
    "    content: str\n",
    "    tokens: int\n",
    "    retrieval_cost_ms: float  # Simulated retrieval latency\n",
    "\n",
    "class JITContextManager:\n",
    "    \"\"\"Manages just-in-time context retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.available_context = self._initialize_context()\n",
    "        self.stats = {\n",
    "            'retrievals': 0,\n",
    "            'tokens_loaded': 0,\n",
    "            'retrieval_time_ms': 0\n",
    "        }\n",
    "    \n",
    "    def _initialize_context(self) -> Dict[str, List[ContextItem]]:\n",
    "        \"\"\"Initialize available context items for SupportMax Pro\"\"\"\n",
    "        return {\n",
    "            'customer_history': [\n",
    "                ContextItem(\"hist_001\", \"customer_history\", \"Past 200 tickets\", 15000, 50),\n",
    "                ContextItem(\"hist_002\", \"customer_history\", \"Subscription details\", 800, 10),\n",
    "                ContextItem(\"hist_003\", \"customer_history\", \"Product usage stats\", 1200, 15)\n",
    "            ],\n",
    "            'knowledge_base': [\n",
    "                ContextItem(\"kb_001\", \"knowledge_base\", \"Export troubleshooting guide\", 3500, 30),\n",
    "                ContextItem(\"kb_002\", \"knowledge_base\", \"Authentication docs\", 2800, 25),\n",
    "                ContextItem(\"kb_003\", \"knowledge_base\", \"Performance optimization\", 4200, 35)\n",
    "            ],\n",
    "            'system_config': [\n",
    "                ContextItem(\"cfg_001\", \"system_config\", \"Product configuration\", 2500, 20),\n",
    "                ContextItem(\"cfg_002\", \"system_config\", \"Integration settings\", 1800, 18)\n",
    "            ],\n",
    "            'error_definitions': [\n",
    "                ContextItem(\"err_001\", \"error_definitions\", \"Error 5032 definition\", 200, 5),\n",
    "                ContextItem(\"err_002\", \"error_definitions\", \"Timeout error patterns\", 350, 8)\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def static_load(self) -> Tuple[int, float]:\n",
    "        \"\"\"Load all context upfront (traditional approach)\"\"\"\n",
    "        total_tokens = 0\n",
    "        total_time = 0\n",
    "        \n",
    "        for category, items in self.available_context.items():\n",
    "            for item in items:\n",
    "                total_tokens += item.tokens\n",
    "                total_time += item.retrieval_cost_ms\n",
    "        \n",
    "        return total_tokens, total_time\n",
    "    \n",
    "    def jit_retrieve(self, needed_items: List[str]) -> Tuple[int, float]:\n",
    "        \"\"\"Retrieve only needed items on-demand\"\"\"\n",
    "        total_tokens = 0\n",
    "        total_time = 0\n",
    "        \n",
    "        for category, items in self.available_context.items():\n",
    "            for item in items:\n",
    "                if item.id in needed_items:\n",
    "                    total_tokens += item.tokens\n",
    "                    total_time += item.retrieval_cost_ms\n",
    "                    self.stats['retrievals'] += 1\n",
    "        \n",
    "        self.stats['tokens_loaded'] = total_tokens\n",
    "        self.stats['retrieval_time_ms'] = total_time\n",
    "        \n",
    "        return total_tokens, total_time\n",
    "    \n",
    "    def hybrid_load(self, needed_items: List[str]) -> Tuple[int, float]:\n",
    "        \"\"\"Load minimal upfront + JIT for specifics\"\"\"\n",
    "        # Upfront: Load minimal customer context\n",
    "        upfront_tokens = 0\n",
    "        upfront_time = 0\n",
    "        \n",
    "        upfront_items = [\"hist_002\", \"err_001\"]  # Subscription + current error\n",
    "        \n",
    "        for category, items in self.available_context.items():\n",
    "            for item in items:\n",
    "                if item.id in upfront_items:\n",
    "                    upfront_tokens += item.tokens\n",
    "                    upfront_time += item.retrieval_cost_ms\n",
    "        \n",
    "        # JIT: Load remaining needed items\n",
    "        jit_items = [item for item in needed_items if item not in upfront_items]\n",
    "        jit_tokens, jit_time = self.jit_retrieve(jit_items)\n",
    "        \n",
    "        return upfront_tokens + jit_tokens, upfront_time + jit_time\n",
    "\n",
    "# Demonstrate JIT retrieval with SupportMax Pro scenario\n",
    "print(\"=== Just-In-Time Context Retrieval Demonstration ===\")\n",
    "print(\"\\nScenario: Customer reports export timeout error\\n\")\n",
    "\n",
    "manager = JITContextManager()\n",
    "\n",
    "# Simulate agent reasoning to determine needed context\n",
    "print(\"Agent reasoning:\")\n",
    "print(\"1. Customer mentioned 'export timeout' ‚Üí Need error definitions\")\n",
    "print(\"2. Need to check if this is recurring ‚Üí Need customer history\")\n",
    "print(\"3. May need troubleshooting steps ‚Üí Need knowledge base article\")\n",
    "print()\n",
    "\n",
    "# Items actually needed for this specific query\n",
    "needed_for_query = [\n",
    "    \"err_001\",  # Error definition\n",
    "    \"err_002\",  # Timeout patterns\n",
    "    \"hist_001\", # Customer history to check for patterns\n",
    "    \"kb_001\",   # Export troubleshooting\n",
    "]\n",
    "\n",
    "# Compare strategies\n",
    "results = {}\n",
    "\n",
    "# Static loading\n",
    "static_tokens, static_time = manager.static_load()\n",
    "results['Static'] = {'tokens': static_tokens, 'time': static_time}\n",
    "\n",
    "# JIT loading\n",
    "jit_tokens, jit_time = manager.jit_retrieve(needed_for_query)\n",
    "results['JIT'] = {'tokens': jit_tokens, 'time': jit_time}\n",
    "\n",
    "# Hybrid loading\n",
    "hybrid_tokens, hybrid_time = manager.hybrid_load(needed_for_query)\n",
    "results['Hybrid'] = {'tokens': hybrid_tokens, 'time': hybrid_time}\n",
    "\n",
    "# Display comparison\n",
    "print(\"=\"*70)\n",
    "print(\"CONTEXT RETRIEVAL STRATEGY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results['token_savings_%'] = ((results['Static']['tokens'] - df_results['tokens']) / \n",
    "                                  results['Static']['tokens'] * 100)\n",
    "df_results['time_savings_%'] = ((results['Static']['time'] - df_results['time']) / \n",
    "                                results['Static']['time'] * 100)\n",
    "\n",
    "print(\"\\n\", df_results.to_string())\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "strategies = list(results.keys())\n",
    "token_vals = [results[s]['tokens'] for s in strategies]\n",
    "time_vals = [results[s]['time'] for s in strategies]\n",
    "\n",
    "# Plot 1: Token efficiency\n",
    "colors = ['#e74c3c', '#2ecc71', '#3498db']\n",
    "bars1 = ax1.bar(strategies, token_vals, color=colors, alpha=0.7)\n",
    "ax1.set_ylabel('Tokens Loaded', fontsize=12)\n",
    "ax1.set_title('Token Efficiency by Strategy\\nFor Single Query', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.yaxis.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, count in zip(bars1, token_vals):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{count:,}\\ntokens',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Retrieval time\n",
    "bars2 = ax2.bar(strategies, time_vals, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Retrieval Time (ms)', fontsize=12)\n",
    "ax2.set_title('Retrieval Latency by Strategy', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.yaxis.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, time in zip(bars2, time_vals):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{time:.0f}ms',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate real-world impact\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REAL-WORLD IMPACT FOR SUPPORTMAX PRO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "daily_queries = 10000\n",
    "jit_savings_tokens = (static_tokens - jit_tokens) * daily_queries\n",
    "jit_savings_time = (static_time - jit_time) * daily_queries / 1000  # Convert to seconds\n",
    "\n",
    "print(f\"\\nWith {daily_queries:,} queries per day:\")\n",
    "print(f\"\\nJIT Strategy savings vs Static:\")\n",
    "print(f\"  Token savings:  {jit_savings_tokens:,} tokens/day ({jit_savings_tokens/1_000_000:.1f}M)\")\n",
    "print(f\"  Time savings:   {jit_savings_time:,.0f} seconds/day ({jit_savings_time/3600:.1f} hours)\")\n",
    "print(f\"\\nCost impact (at $10 per 1M tokens):\")\n",
    "print(f\"  Daily savings:  ${jit_savings_tokens/1_000_000 * 10:.2f}\")\n",
    "print(f\"  Annual savings: ${jit_savings_tokens/1_000_000 * 10 * 365:,.2f}\")\n",
    "\n",
    "print(\"\\nüí° JIT retrieval reduces token usage by 62% while maintaining response quality!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Multi-modal Context Handling\n",
    "\n",
    "### Processing Screenshots, Logs, and Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import io\n",
    "import base64\n",
    "\n",
    "class MultiModalContextProcessor:\n",
    "    \"\"\"Processes different modalities for SupportMax Pro\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def simulate_screenshot_analysis(error_message: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Simulates vision model analyzing a screenshot\n",
    "        In production, this would use GPT-4 Vision, Claude 3, or Gemini\n",
    "        \"\"\"\n",
    "        # Simulate extraction of visual information\n",
    "        extracted_info = {\n",
    "            'error_code': None,\n",
    "            'error_message': error_message,\n",
    "            'ui_state': 'export_dialog',\n",
    "            'browser': 'Chrome',\n",
    "            'os': 'Windows',\n",
    "            'timestamp_visible': True,\n",
    "            'user_action': 'attempting_export'\n",
    "        }\n",
    "        \n",
    "        # Extract error code if present\n",
    "        if 'Error' in error_message and ':' in error_message:\n",
    "            parts = error_message.split(':')\n",
    "            error_code = parts[0].replace('Error', '').strip()\n",
    "            extracted_info['error_code'] = error_code\n",
    "        \n",
    "        return extracted_info\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_log_file(log_entries: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract structured information from log entries\n",
    "        \"\"\"\n",
    "        processed = {\n",
    "            'total_entries': len(log_entries),\n",
    "            'errors': [],\n",
    "            'warnings': [],\n",
    "            'timeline': [],\n",
    "            'patterns': []\n",
    "        }\n",
    "        \n",
    "        for entry in log_entries:\n",
    "            # Parse timestamp\n",
    "            if ' - ' in entry:\n",
    "                timestamp, message = entry.split(' - ', 1)\n",
    "                processed['timeline'].append({'time': timestamp, 'event': message})\n",
    "                \n",
    "                # Categorize\n",
    "                if 'ERROR' in message.upper() or 'FAIL' in message.upper():\n",
    "                    processed['errors'].append(message)\n",
    "                elif 'WARN' in message.upper():\n",
    "                    processed['warnings'].append(message)\n",
    "        \n",
    "        # Identify patterns\n",
    "        if len(processed['errors']) > 0:\n",
    "            if any('timeout' in e.lower() for e in processed['errors']):\n",
    "                processed['patterns'].append('Timeout pattern detected')\n",
    "            if any('connection' in e.lower() for e in processed['errors']):\n",
    "                processed['patterns'].append('Connection issues detected')\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_fused_context(screenshot_info: Dict, log_info: Dict, \n",
    "                           customer_description: str) -> str:\n",
    "        \"\"\"\n",
    "        Fuse multi-modal information into coherent context\n",
    "        \"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        # Customer's description\n",
    "        context_parts.append(f\"Customer Description: {customer_description}\")\n",
    "        \n",
    "        # Screenshot analysis\n",
    "        if screenshot_info.get('error_code'):\n",
    "            context_parts.append(f\"\\nVisual Analysis:\")\n",
    "            context_parts.append(f\"  Error Code: {screenshot_info['error_code']}\")\n",
    "            context_parts.append(f\"  Error Message: {screenshot_info['error_message']}\")\n",
    "            context_parts.append(f\"  UI State: {screenshot_info['ui_state']}\")\n",
    "            context_parts.append(f\"  Environment: {screenshot_info['browser']} on {screenshot_info['os']}\")\n",
    "        \n",
    "        # Log analysis\n",
    "        if log_info:\n",
    "            context_parts.append(f\"\\nLog Analysis:\")\n",
    "            context_parts.append(f\"  Total entries: {log_info['total_entries']}\")\n",
    "            context_parts.append(f\"  Errors: {len(log_info['errors'])}\")\n",
    "            context_parts.append(f\"  Warnings: {len(log_info['warnings'])}\")\n",
    "            \n",
    "            if log_info['patterns']:\n",
    "                context_parts.append(f\"  Patterns: {', '.join(log_info['patterns'])}\")\n",
    "            \n",
    "            # Key timeline events\n",
    "            if log_info['timeline']:\n",
    "                context_parts.append(f\"\\n  Timeline:\")\n",
    "                for event in log_info['timeline'][:3]:  # Show first 3 events\n",
    "                    context_parts.append(f\"    {event['time']}: {event['event']}\")\n",
    "        \n",
    "        # Cross-modal correlation\n",
    "        context_parts.append(f\"\\nCross-Modal Analysis:\")\n",
    "        \n",
    "        # Correlate screenshot error with logs\n",
    "        if screenshot_info.get('error_code') and log_info.get('errors'):\n",
    "            error_code = screenshot_info['error_code']\n",
    "            matching_logs = [e for e in log_info['errors'] if error_code in e]\n",
    "            if matching_logs:\n",
    "                context_parts.append(f\"  ‚úì Screenshot error {error_code} confirmed in logs\")\n",
    "                context_parts.append(f\"    Log entry: {matching_logs[0]}\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "\n",
    "# Demonstrate multi-modal processing\n",
    "print(\"=== Multi-Modal Context Processing for SupportMax Pro ===\")\n",
    "print(\"\\nScenario: Customer reports issue with screenshot and logs\\n\")\n",
    "\n",
    "processor = MultiModalContextProcessor()\n",
    "\n",
    "# Customer's text description\n",
    "customer_description = \"I'm getting an error when trying to export our customer database. The export starts but then fails after a few minutes.\"\n",
    "\n",
    "# Simulate screenshot showing error message\n",
    "error_in_screenshot = \"Error 5032: Export timeout after 300 seconds\"\n",
    "screenshot_analysis = processor.simulate_screenshot_analysis(error_in_screenshot)\n",
    "\n",
    "# Process log file\n",
    "log_entries = [\n",
    "    \"2025-04-10 14:32:15 - Export initiated for 2.3M records\",\n",
    "    \"2025-04-10 14:35:30 - Processing batch 1 of 5\",\n",
    "    \"2025-04-10 14:37:42 - Processing 47% complete\",\n",
    "    \"2025-04-10 14:37:50 - WARNING: Database connection slow\",\n",
    "    \"2025-04-10 14:38:01 - ERROR 5032: Connection timeout after 300s\",\n",
    "    \"2025-04-10 14:38:02 - Export failed, rolling back transaction\"\n",
    "]\n",
    "log_analysis = processor.process_log_file(log_entries)\n",
    "\n",
    "# Create fused context\n",
    "fused_context = processor.create_fused_context(\n",
    "    screenshot_analysis, \n",
    "    log_analysis, \n",
    "    customer_description\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FUSED MULTI-MODAL CONTEXT\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(fused_context)\n",
    "print()\n",
    "\n",
    "# Compare with text-only approach\n",
    "text_only_context = f\"Customer Description: {customer_description}\"\n",
    "\n",
    "counter = TokenCounter()\n",
    "text_only_tokens = counter.count(text_only_context)\n",
    "fused_tokens = counter.count(fused_context)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONTEXT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_data = {\n",
    "    'Approach': ['Text-Only', 'Multi-Modal Fusion'],\n",
    "    'Tokens': [text_only_tokens, fused_tokens],\n",
    "    'Information Richness': ['Low', 'High'],\n",
    "    'Diagnostic Value': ['Limited', 'Comprehensive']\n",
    "}\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\", df.to_string(index=False))\n",
    "\n",
    "# Visualize information extraction\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Create a flow diagram showing multi-modal fusion\n",
    "ax.text(0.5, 0.95, 'Multi-Modal Context Fusion Pipeline', \n",
    "        ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Input sources\n",
    "inputs = [\n",
    "    {'x': 0.15, 'y': 0.75, 'label': 'Customer\\nDescription', 'color': '#3498db'},\n",
    "    {'x': 0.5, 'y': 0.75, 'label': 'Screenshot\\nAnalysis', 'color': '#e74c3c'},\n",
    "    {'x': 0.85, 'y': 0.75, 'label': 'Log File\\nProcessing', 'color': '#2ecc71'}\n",
    "]\n",
    "\n",
    "for inp in inputs:\n",
    "    circle = plt.Circle((inp['x'], inp['y']), 0.08, color=inp['color'], alpha=0.7)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(inp['x'], inp['y'], inp['label'], ha='center', va='center', \n",
    "           fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "# Processing layer\n",
    "ax.text(0.5, 0.5, 'Cross-Modal\\nCorrelation Engine', \n",
    "       ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='#f39c12', alpha=0.7, pad=0.5))\n",
    "\n",
    "# Arrows from inputs to processing\n",
    "for inp in inputs:\n",
    "    ax.annotate('', xy=(0.5, 0.55), xytext=(inp['x'], inp['y']-0.08),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "\n",
    "# Output\n",
    "ax.text(0.5, 0.25, 'Enriched Context\\nfor Agent Reasoning', \n",
    "       ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round', facecolor='#9b59b6', alpha=0.7, pad=0.5))\n",
    "\n",
    "# Arrow to output\n",
    "ax.annotate('', xy=(0.5, 0.3), xytext=(0.5, 0.45),\n",
    "           arrowprops=dict(arrowstyle='->', lw=3, color='black'))\n",
    "\n",
    "# Add extracted insights\n",
    "insights = [\n",
    "    'Error code: 5032',\n",
    "    'Timeout pattern',\n",
    "    'Database connection issue',\n",
    "    '2.3M record dataset',\n",
    "    'Timeline correlation'\n",
    "]\n",
    "\n",
    "ax.text(0.05, 0.1, 'Extracted Insights:', fontsize=10, fontweight='bold')\n",
    "for i, insight in enumerate(insights):\n",
    "    ax.text(0.07, 0.07 - i*0.02, f'‚Ä¢ {insight}', fontsize=9)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Multi-modal fusion provides 5x more diagnostic information!\")\n",
    "print(\"   Agent can immediately understand:\")\n",
    "print(\"   - Exact error code and timing\")\n",
    "print(\"   - Root cause (database connection timeout)\")\n",
    "print(\"   - Dataset size correlation\")\n",
    "print(\"   - Customer environment details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Context Health Monitoring\n",
    "\n",
    "### Production Observability Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextHealthMonitor:\n",
    "    \"\"\"Monitors context health metrics for production systems\"\"\"\n",
    "    \n",
    "    def __init__(self, context_budget: int = 128000):\n",
    "        self.context_budget = context_budget\n",
    "        self.metrics_history = []\n",
    "    \n",
    "    def measure_context_health(self, context: str, \n",
    "                               conversation_tokens: int,\n",
    "                               relevant_items: int,\n",
    "                               total_items: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Measure key context health metrics\n",
    "        \"\"\"\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        total_tokens = len(encoding.encode(context))\n",
    "        \n",
    "        metrics = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'context_utilization': total_tokens / self.context_budget,\n",
    "            'relevance_score': relevant_items / total_items if total_items > 0 else 0,\n",
    "            'compression_ratio': 1 - (conversation_tokens / total_tokens) if total_tokens > 0 else 0,\n",
    "            'total_tokens': total_tokens,\n",
    "            'available_budget': self.context_budget - total_tokens,\n",
    "            'health_status': 'unknown'\n",
    "        }\n",
    "        \n",
    "        # Determine health status\n",
    "        if metrics['context_utilization'] > 0.9:\n",
    "            metrics['health_status'] = 'critical'  # Over 90% usage\n",
    "        elif metrics['context_utilization'] > 0.75:\n",
    "            metrics['health_status'] = 'warning'   # Over 75% usage\n",
    "        elif metrics['relevance_score'] < 0.6:\n",
    "            metrics['health_status'] = 'warning'   # Low relevance\n",
    "        else:\n",
    "            metrics['health_status'] = 'healthy'\n",
    "        \n",
    "        self.metrics_history.append(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def visualize_health_dashboard(self):\n",
    "        \"\"\"Create a health monitoring dashboard\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            print(\"No metrics to display\")\n",
    "            return\n",
    "        \n",
    "        fig = plt.figure(figsize=(16, 10))\n",
    "        gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Extract metrics over time\n",
    "        utilization = [m['context_utilization'] for m in self.metrics_history]\n",
    "        relevance = [m['relevance_score'] for m in self.metrics_history]\n",
    "        tokens = [m['total_tokens'] for m in self.metrics_history]\n",
    "        timestamps = [m['timestamp'] for m in self.metrics_history]\n",
    "        \n",
    "        # Plot 1: Context Utilization Over Time\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        ax1.plot(range(len(utilization)), [u*100 for u in utilization], \n",
    "                'b-', linewidth=2, marker='o')\n",
    "        ax1.axhline(y=75, color='orange', linestyle='--', label='Warning Threshold (75%)')\n",
    "        ax1.axhline(y=90, color='red', linestyle='--', label='Critical Threshold (90%)')\n",
    "        ax1.fill_between(range(len(utilization)), [u*100 for u in utilization], \n",
    "                        alpha=0.3)\n",
    "        ax1.set_ylabel('Utilization (%)', fontsize=12)\n",
    "        ax1.set_title('Context Budget Utilization Over Time', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Relevance Score\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        colors = ['green' if r >= 0.7 else 'orange' if r >= 0.5 else 'red' \n",
    "                 for r in relevance]\n",
    "        ax2.bar(range(len(relevance)), [r*100 for r in relevance], \n",
    "               color=colors, alpha=0.7)\n",
    "        ax2.axhline(y=60, color='red', linestyle='--', alpha=0.5)\n",
    "        ax2.set_ylabel('Relevance (%)', fontsize=12)\n",
    "        ax2.set_title('Context Relevance Score', fontsize=14, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Plot 3: Token Usage\n",
    "        ax3 = fig.add_subplot(gs[1, 1])\n",
    "        ax3.plot(range(len(tokens)), tokens, 'g-', linewidth=2, marker='s')\n",
    "        ax3.axhline(y=self.context_budget, color='red', linestyle='--', \n",
    "                   label=f'Budget Limit ({self.context_budget:,})')\n",
    "        ax3.fill_between(range(len(tokens)), tokens, alpha=0.3)\n",
    "        ax3.set_ylabel('Total Tokens', fontsize=12)\n",
    "        ax3.set_title('Token Consumption', fontsize=14, fontweight='bold')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Health Status Distribution\n",
    "        ax4 = fig.add_subplot(gs[2, :])\n",
    "        status_counts = pd.Series([m['health_status'] for m in self.metrics_history]).value_counts()\n",
    "        status_colors = {'healthy': '#2ecc71', 'warning': '#f39c12', 'critical': '#e74c3c'}\n",
    "        colors_list = [status_colors.get(status, '#95a5a6') for status in status_counts.index]\n",
    "        \n",
    "        bars = ax4.bar(status_counts.index, status_counts.values, \n",
    "                      color=colors_list, alpha=0.7)\n",
    "        ax4.set_ylabel('Count', fontsize=12)\n",
    "        ax4.set_title('Context Health Status Distribution', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        for bar, count in zip(bars, status_counts.values):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(count)}',\n",
    "                    ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle('SupportMax Pro - Context Health Dashboard', \n",
    "                    fontsize=16, fontweight='bold', y=0.995)\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate context health monitoring\n",
    "print(\"=== Context Health Monitoring Demonstration ===\")\n",
    "print(\"\\nSimulating SupportMax Pro context health over 20 customer interactions\\n\")\n",
    "\n",
    "monitor = ContextHealthMonitor(context_budget=128000)\n",
    "\n",
    "# Simulate 20 interactions with varying context characteristics\n",
    "for i in range(20):\n",
    "    # Simulate varying context sizes and relevance\n",
    "    if i < 5:\n",
    "        # Early interactions - efficient context\n",
    "        context_size = random.randint(8000, 15000)\n",
    "        conversation_tokens = random.randint(2000, 4000)\n",
    "        relevant_items = random.randint(8, 10)\n",
    "        total_items = 10\n",
    "    elif i < 15:\n",
    "        # Middle interactions - growing context\n",
    "        context_size = random.randint(15000, 40000)\n",
    "        conversation_tokens = random.randint(3000, 6000)\n",
    "        relevant_items = random.randint(6, 9)\n",
    "        total_items = 12\n",
    "    else:\n",
    "        # Later interactions - potential issues\n",
    "        context_size = random.randint(40000, 100000)\n",
    "        conversation_tokens = random.randint(5000, 10000)\n",
    "        relevant_items = random.randint(5, 8)\n",
    "        total_items = 15\n",
    "    \n",
    "    # Create dummy context\n",
    "    context = \"x\" * context_size\n",
    "    \n",
    "    # Measure health\n",
    "    metrics = monitor.measure_context_health(\n",
    "        context, \n",
    "        conversation_tokens, \n",
    "        relevant_items, \n",
    "        total_items\n",
    "    )\n",
    "    \n",
    "    print(f\"Interaction {i+1:2d}: Utilization={metrics['context_utilization']*100:5.1f}% | \"\n",
    "          f\"Relevance={metrics['relevance_score']*100:5.1f}% | \"\n",
    "          f\"Status={metrics['health_status']:8} | \"\n",
    "          f\"Tokens={metrics['total_tokens']:6,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZING HEALTH METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "monitor.visualize_health_dashboard()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HEALTH SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "avg_utilization = np.mean([m['context_utilization'] for m in monitor.metrics_history])\n",
    "avg_relevance = np.mean([m['relevance_score'] for m in monitor.metrics_history])\n",
    "max_tokens = max([m['total_tokens'] for m in monitor.metrics_history])\n",
    "\n",
    "status_counts = pd.Series([m['health_status'] for m in monitor.metrics_history]).value_counts()\n",
    "\n",
    "print(f\"\\nAverage Utilization: {avg_utilization*100:.1f}%\")\n",
    "print(f\"Average Relevance:   {avg_relevance*100:.1f}%\")\n",
    "print(f\"Peak Token Usage:    {max_tokens:,} ({max_tokens/128000*100:.1f}% of budget)\")\n",
    "print(f\"\\nHealth Status Breakdown:\")\n",
    "for status, count in status_counts.items():\n",
    "    print(f\"  {status.capitalize():10} : {count} ({count/len(monitor.metrics_history)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nüí° Context health monitoring enables proactive optimization!\")\n",
    "print(\"   - Detect context bloat before it impacts performance\")\n",
    "print(\"   - Identify low-relevance content for pruning\")\n",
    "print(\"   - Track trends to prevent context budget exhaustion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Complete SupportMax Pro Enhancement\n",
    "\n",
    "### Before and After Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SupportMax Pro: Architecture v1 vs v2 Comparison ===\")\n",
    "print(\"\\nDemonstrating the impact of context engineering techniques\\n\")\n",
    "\n",
    "# Scenario setup\n",
    "scenario = {\n",
    "    'customer': 'Acme Corp',\n",
    "    'history_tickets': 200,\n",
    "    'relationship_years': 3,\n",
    "    'current_issue': 'Export timeout with large dataset'\n",
    "}\n",
    "\n",
    "print(\"Scenario:\")\n",
    "for key, value in scenario.items():\n",
    "    print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "# Architecture v1 metrics (before context engineering)\n",
    "v1_metrics = {\n",
    "    'strategy': 'Load all data upfront',\n",
    "    'context_approach': 'Static, comprehensive',\n",
    "    'total_tokens': 88000,\n",
    "    'response_time_ms': 4200,\n",
    "    'cost_per_query': 1.85,\n",
    "    'exchanges_before_exhaustion': 3,\n",
    "    'resolution_time_minutes': 35,\n",
    "    'total_exchanges': 15,\n",
    "    'customer_satisfaction': 3.2\n",
    "}\n",
    "\n",
    "# Architecture v2 metrics (after context engineering)\n",
    "v2_metrics = {\n",
    "    'strategy': 'JIT retrieval + compression',\n",
    "    'context_approach': 'Dynamic, optimized',\n",
    "    'total_tokens': 11300,\n",
    "    'response_time_ms': 800,\n",
    "    'cost_per_query': 0.35,\n",
    "    'exchanges_before_exhaustion': 20,\n",
    "    'resolution_time_minutes': 8,\n",
    "    'total_exchanges': 4,\n",
    "    'customer_satisfaction': 4.7\n",
    "}\n",
    "\n",
    "# Calculate improvements\n",
    "improvements = {\n",
    "    'Token Reduction': ((v1_metrics['total_tokens'] - v2_metrics['total_tokens']) / \n",
    "                       v1_metrics['total_tokens'] * 100),\n",
    "    'Speed Improvement': ((v1_metrics['response_time_ms'] - v2_metrics['response_time_ms']) / \n",
    "                         v1_metrics['response_time_ms'] * 100),\n",
    "    'Cost Savings': ((v1_metrics['cost_per_query'] - v2_metrics['cost_per_query']) / \n",
    "                    v1_metrics['cost_per_query'] * 100),\n",
    "    'Resolution Time Reduction': ((v1_metrics['resolution_time_minutes'] - \n",
    "                                  v2_metrics['resolution_time_minutes']) / \n",
    "                                 v1_metrics['resolution_time_minutes'] * 100),\n",
    "    'CSAT Improvement': ((v2_metrics['customer_satisfaction'] - \n",
    "                         v1_metrics['customer_satisfaction']) / \n",
    "                        v1_metrics['customer_satisfaction'] * 100)\n",
    "}\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Total Tokens', 'Response Time (ms)', 'Cost per Query ($)', \n",
    "               'Resolution Time (min)', 'Total Exchanges', 'Customer Satisfaction'],\n",
    "    'Architecture v1': [\n",
    "        f\"{v1_metrics['total_tokens']:,}\",\n",
    "        v1_metrics['response_time_ms'],\n",
    "        f\"${v1_metrics['cost_per_query']:.2f}\",\n",
    "        v1_metrics['resolution_time_minutes'],\n",
    "        v1_metrics['total_exchanges'],\n",
    "        f\"{v1_metrics['customer_satisfaction']:.1f}/5\"\n",
    "    ],\n",
    "    'Architecture v2': [\n",
    "        f\"{v2_metrics['total_tokens']:,}\",\n",
    "        v2_metrics['response_time_ms'],\n",
    "        f\"${v2_metrics['cost_per_query']:.2f}\",\n",
    "        v2_metrics['resolution_time_minutes'],\n",
    "        v2_metrics['total_exchanges'],\n",
    "        f\"{v2_metrics['customer_satisfaction']:.1f}/5\"\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        f\"{improvements['Token Reduction']:.1f}% ‚Üì\",\n",
    "        f\"{improvements['Speed Improvement']:.1f}% ‚Üì\",\n",
    "        f\"{improvements['Cost Savings']:.1f}% ‚Üì\",\n",
    "        f\"{improvements['Resolution Time Reduction']:.1f}% ‚Üì\",\n",
    "        f\"73% ‚Üì\",\n",
    "        f\"{improvements['CSAT Improvement']:.1f}% ‚Üë\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\", comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize improvements\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Token usage\n",
    "versions = ['v1', 'v2']\n",
    "tokens = [v1_metrics['total_tokens'], v2_metrics['total_tokens']]\n",
    "colors = ['#e74c3c', '#2ecc71']\n",
    "\n",
    "bars1 = ax1.bar(versions, tokens, color=colors, alpha=0.7)\n",
    "ax1.set_ylabel('Tokens', fontsize=12)\n",
    "ax1.set_title('Token Usage Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.yaxis.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, count in zip(bars1, tokens):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{count:,}\\n({improvements[\"Token Reduction\"]:.0f}% reduction)',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Response time\n",
    "times = [v1_metrics['response_time_ms'], v2_metrics['response_time_ms']]\n",
    "bars2 = ax2.bar(versions, times, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Response Time (ms)', fontsize=12)\n",
    "ax2.set_title('Response Time Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.yaxis.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, time in zip(bars2, times):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{time}ms\\n({improvements[\"Speed Improvement\"]:.0f}% faster)',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 3: Cost per query\n",
    "costs = [v1_metrics['cost_per_query'], v2_metrics['cost_per_query']]\n",
    "bars3 = ax3.bar(versions, costs, color=colors, alpha=0.7)\n",
    "ax3.set_ylabel('Cost ($)', fontsize=12)\n",
    "ax3.set_title('Cost per Query Comparison', fontsize=14, fontweight='bold')\n",
    "ax3.yaxis.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, cost in zip(bars3, costs):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'${cost:.2f}\\n({improvements[\"Cost Savings\"]:.0f}% savings)',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 4: Overall impact\n",
    "impact_metrics = ['Token\\nReduction', 'Speed\\nImprovement', 'Cost\\nSavings', \n",
    "                 'Resolution\\nTime', 'CSAT\\nIncrease']\n",
    "impact_values = [\n",
    "    improvements['Token Reduction'],\n",
    "    improvements['Speed Improvement'],\n",
    "    improvements['Cost Savings'],\n",
    "    improvements['Resolution Time Reduction'],\n",
    "    improvements['CSAT Improvement']\n",
    "]\n",
    "\n",
    "bars4 = ax4.barh(impact_metrics, impact_values, color='#3498db', alpha=0.7)\n",
    "ax4.set_xlabel('Improvement (%)', fontsize=12)\n",
    "ax4.set_title('Overall Impact of Context Engineering', fontsize=14, fontweight='bold')\n",
    "ax4.xaxis.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, value in zip(bars4, impact_values):\n",
    "    width = bar.get_width()\n",
    "    ax4.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "            f'{value:.0f}%',\n",
    "            ha='left', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('SupportMax Pro: Architecture v1 vs v2\\nImpact of Context Engineering Techniques', \n",
    "            fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate annual business impact\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANNUAL BUSINESS IMPACT (50,000 tickets/month)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "monthly_tickets = 50000\n",
    "annual_tickets = monthly_tickets * 12\n",
    "\n",
    "cost_savings_annual = (v1_metrics['cost_per_query'] - v2_metrics['cost_per_query']) * annual_tickets\n",
    "time_savings_annual_hours = ((v1_metrics['resolution_time_minutes'] - \n",
    "                             v2_metrics['resolution_time_minutes']) * annual_tickets) / 60\n",
    "\n",
    "print(f\"\\nCost Savings:\")\n",
    "print(f\"  Per ticket:  ${v1_metrics['cost_per_query'] - v2_metrics['cost_per_query']:.2f}\")\n",
    "print(f\"  Annual:      ${cost_savings_annual:,.2f}\")\n",
    "\n",
    "print(f\"\\nTime Savings:\")\n",
    "print(f\"  Per ticket:  {v1_metrics['resolution_time_minutes'] - v2_metrics['resolution_time_minutes']} minutes\")\n",
    "print(f\"  Annual:      {time_savings_annual_hours:,.0f} hours ({time_savings_annual_hours/8:,.0f} work days)\")\n",
    "\n",
    "print(f\"\\nCustomer Satisfaction:\")\n",
    "print(f\"  Improvement: {v2_metrics['customer_satisfaction'] - v1_metrics['customer_satisfaction']:.1f} points\")\n",
    "print(f\"  New rating:  {v2_metrics['customer_satisfaction']:.1f}/5 ({v2_metrics['customer_satisfaction']/5*100:.0f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nContext engineering transforms SupportMax Pro performance:\")\n",
    "print(\"  ‚úì 87% reduction in token usage\")\n",
    "print(\"  ‚úì 81% faster response times\")\n",
    "print(\"  ‚úì 77% faster resolution\")\n",
    "print(f\"  ‚úì ${cost_savings_annual:,.0f} annual cost savings\")\n",
    "print(\"  ‚úì Significantly improved customer satisfaction\")\n",
    "print(\"\\nThese techniques enable scaling to 100,000+ tickets/month!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion and Next Steps\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "In this notebook, we've demonstrated the key concepts from Chapter 5:\n",
    "\n",
    "1. **Context Engineering Foundations**: Strategic budgeting and optimal ordering\n",
    "2. **Ontologies and Knowledge Graphs**: Building semantic layers for enterprise context\n",
    "3. **Context Window Optimization**: Format choices, caching strategies, and breakpoints\n",
    "4. **Compression Techniques**: Semantic vs aggressive compression trade-offs\n",
    "5. **JIT Retrieval**: Dynamic context loading for efficiency\n",
    "6. **Multi-modal Processing**: Fusing text, images, and logs\n",
    "7. **Health Monitoring**: Production observability for context systems\n",
    "8. **Real-World Impact**: Before/after comparison with SupportMax Pro\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Context engineering is as important as model selection\n",
    "- Proper context management can reduce costs by 60-80%\n",
    "- JIT retrieval outperforms static loading in most scenarios\n",
    "- Semantic compression preserves information better than aggressive compression\n",
    "- Multi-modal fusion provides significantly richer context\n",
    "- Production systems require continuous context health monitoring\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To apply these concepts to your own system:\n",
    "\n",
    "1. Audit your current context usage patterns\n",
    "2. Implement token counting and budgeting\n",
    "3. Build a knowledge graph for your domain\n",
    "4. Add prompt caching to reduce costs\n",
    "5. Implement semantic compression for long conversations\n",
    "6. Add health monitoring to detect context issues early\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- Chapter 6: Production Memory Implementation\n",
    "- Chapter 12: Observability and Security\n",
    "- Chapter 14-16: Cloud-specific implementations\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for exploring Chapter 5 concepts with SupportMax Pro!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
